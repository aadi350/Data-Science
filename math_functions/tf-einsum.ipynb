{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unravelling `tf.einsum`\n",
    "## Origin Story\n",
    "Recently, I was trying to disect the original [DCNN Paper](https://arxiv.org/abs/1511.02136v6) which utilized a *diffusion* kernel to more readily make use of implicit graph-structure in common tasks such as node, edge and graph classification. However, an existing implementation I fonund had a curious piece of notation which led me down the rabbithole of Tensor calculus.\n",
    "\n",
    "Coordinates are maps used to solve a given problem. A coordinate transform allows mapping from one frame of reference to another (converting from a map of your high school, to the location of your high school in reference to where it is in the city, compared to a country-wide map).\n",
    "\n",
    "To say a number is a sclar means that the value does no change when transformed from one coordinate system to another (e.g. the distance between two points on a flat plain is irrespective of where true north is).\n",
    "\n",
    "A vector is directional, and can be formed on the basis of the reference set of coordinates. For example, a vector between your home and the nearest fire-station can be broken down into a sum of north- and east-facing vectors.\n",
    "\n",
    "## Tensors\n",
    "A tensor describes the superset of transformations which include scalars and vectors:  \n",
    "- $0$-tensors are constant functions, which we identify as scalars  \n",
    "- $1$-tensors are linear functions, which we call vectors  \n",
    "- $2$-tensors are bilinear functions, which we call matrices \n",
    "\n",
    "A **Tensor** describes any general transformation, independent of any basis function between sets of algebraic objects related to a vector space\n",
    "\n",
    "---\n",
    "\n",
    "Back to the paper, there was a particular function which claimed to do batch matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.einsum('ijk,kl->ijl', A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $A$ was the diffusion kernel and $B$ was a feature vector. So $A$ would have dimensions (batch_size, m, n) and $B$ would have dimensions (n, k), where:\n",
    "- batch_size: number of nodes to process in a given batch (for model trainining)\n",
    "- n: number of features \n",
    "- m: number of nodes \n",
    "- k: number of \"hops\"\n",
    "\n",
    "Ignoring the technicalities of the paper and the actual definitions above, I wanted to know what the actual heck this strange `einsum` function was trying to do\n",
    "\n",
    "## Einstein Summation\n",
    "\n",
    "Enter *Einstein* summation: \n",
    "In \"Einstein\" summation, the repeated index defines what we sum by, the expression must have a repeated index, so:\n",
    "$$\n",
    "\\sum_{i=1}^n = a_1x_1 + a_2x_2 + ... + a_nx_n \\equiv a_ix_i\n",
    "$$\n",
    "is valid. But $a_{ij}x_k$ is not, whilst $a_{ij}x_j$ is:\n",
    "$$\n",
    "a_{ij}x_j \\equiv a_{i1}x_1 + a_{i2}x_2 + ... + a_{in}x_n\n",
    "$$\n",
    "\n",
    "Double sums are handled as follows, for example summation on both $i$ and $j$:\n",
    "$$\n",
    "a_{ij}x_iy_j\n",
    "$$\n",
    "\n",
    "In the `einsum` function, the first argument `ijk,kl->ijl` signified summation on the $k^{th}$ dimension\n",
    "\n",
    "---\n",
    "\n",
    "Now that I understood what the notation meant, I wanted a better grasp on the actual mechanics behind the function. Using synthetic Tensors as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 2), dtype=int32, numpy=\n",
       "array([[[14,  4],\n",
       "        [ 4, 12],\n",
       "        [ 9, 13],\n",
       "        [ 0, 13]]], dtype=int32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 2\n",
    "batch_size, m, n = None, 4, 2\n",
    "init = tf.random.uniform(shape=(m, n), minval=0, maxval=16, dtype=tf.int32)\n",
    "A = tf.Variable(init)\n",
    "A = tf.expand_dims(A, 0)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
       "array([[3, 9],\n",
       "       [5, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.random.uniform(shape=(n, k), minval=0, maxval=16, dtype=tf.int32)\n",
    "B = tf.Variable(init)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.matmul`  \n",
    "Here is where I used the two prior defined Tensors to basically see what would happen. It was also at this point I realised that TensorFlow 2 now included a function `matmul` which essentially did the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 2), dtype=int32, numpy=\n",
       "array([[[ 62, 130],\n",
       "        [ 72,  48],\n",
       "        [ 92,  94],\n",
       "        [ 65,  13]]], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.einsum('ijk,kl->ijl', A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 2), dtype=int32, numpy=\n",
       "array([[[ 62, 130],\n",
       "        [ 72,  48],\n",
       "        [ 92,  94],\n",
       "        [ 65,  13]]], dtype=int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum-Viable Example\n",
    "\n",
    "Okay, now simplifying even further; firstly by creating a rank-2 tensor (i.e. a matrix) using numpy and then finding the matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[29, 39],\n",
       "        [28, 38]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix('''\n",
    "    1 4;\n",
    "    2 3\n",
    "''')\n",
    "\n",
    "B = np.matrix('''\n",
    "    5 7;\n",
    "    6 8\n",
    "''')\n",
    "\n",
    "C = A @ B\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every element in $C$, $C_{ik}$ is:\n",
    "$$\n",
    "C_{ik} = \\sum_jA_{ij}B_{jk}\n",
    "$$\n",
    "\n",
    "$C_{01} = 39$ so\n",
    "\n",
    "$$\n",
    "C_{01} = \\sum_jA_{0j}B_{j1} = (1\\times 7)_{j=0} + (4\\times 8)_{j=1}\n",
    "$$\n",
    "\n",
    "Followed by converting the above matrices to TensorFlow objects and repeating the operation to somehow test that I grasped the notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
       " array([[1, 4],\n",
       "        [2, 3]])>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
       " array([[5, 7],\n",
       "        [6, 8]])>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.convert_to_tensor(A)\n",
    "B = tf.convert_to_tensor(B)\n",
    "\n",
    "A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! The output of `einsum` below is consistent with `matmul` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
       "array([[29, 39],\n",
       "       [28, 38]])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent to A @ B or tf.matmul(A, B)\n",
    "tf.einsum('ij,jk->ik', A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly-Less Minimum Example  \n",
    "Now on to a slightly more complex example, I created a rank-2 Tensor and a rank-1 Tensor for multiplication against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 2, 2]), TensorShape([2, 1]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying to batch case\n",
    "A = tf.Variable([\n",
    "    [[1,2],\n",
    "    [3,4]],\n",
    "    [[3, 5], \n",
    "    [2, 9]]\n",
    "])\n",
    "\n",
    "B = tf.Variable(\n",
    "    [[2], [1]]\n",
    ")\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\n",
       "array([[[ 4],\n",
       "        [10]],\n",
       "\n",
       "       [[11],\n",
       "        [13]]], dtype=int32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $ijl^{th}$ element in $C$, sum across the $k^{th}$ dimension in A and B\n",
    "\n",
    "```\n",
    "output[i,j,l] = sum_k A[i,j,k] * B[k, l]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\n",
       "array([[[ 4],\n",
       "        [10]],\n",
       "\n",
       "       [[11],\n",
       "        [13]]], dtype=int32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the ijl-th element in C, \n",
    "C = tf.einsum('ijk,kl->ijl', A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and success! I think I have a fair grasp on how `einsum` and Einstein summation works, and how/why it can be sometimes simpler just to use the built-in `matmul` function, but also where batch dimensions may mess with the built-in functions and we would want to define it in finer detail"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
