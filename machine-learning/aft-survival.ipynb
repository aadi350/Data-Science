{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/09 12:20:02 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.230 instead (on interface wlp5s0)\n",
      "23/06/09 12:20:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/09 12:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/09 13:06:37 WARN TaskSetManager: Stage 1172 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+---+------+------+------+------------------+------+\n",
      "|  a|dummy1|dummy2|dummy3|             label|censor|\n",
      "+---+------+------+------+------------------+------+\n",
      "| 20|     1|     1|     1| 2.995732273553991|     0|\n",
      "| 47|     1|     1|     1|3.8501476017100584|     0|\n",
      "| 63|     1|     1|     1| 4.143134726391533|     0|\n",
      "| 44|     1|     1|     1| 3.784189633918261|     0|\n",
      "| 15|     1|     1|     1|  2.70805020110221|     0|\n",
      "| 17|     1|     1|     1| 2.833213344056216|     0|\n",
      "| 75|     1|     1|     1|  4.31748811353631|     0|\n",
      "| 27|     1|     1|     1| 3.295836866004329|     0|\n",
      "| 53|     1|     1|     1| 3.970291913552122|     0|\n",
      "| 38|     1|     1|     1|3.6375861597263857|     0|\n",
      "| 22|     1|     1|     1| 3.091042453358316|     0|\n",
      "| 98|     1|     1|     1| 4.584967478670572|     0|\n",
      "| 81|     1|     1|     1| 4.394449154672439|     0|\n",
      "| 77|     1|     1|     1| 4.343805421853684|     0|\n",
      "| 48|     1|     1|     1| 3.871201010907891|     0|\n",
      "| 37|     1|     1|     1|3.6109179126442243|     0|\n",
      "| 95|     1|     1|     1| 4.553876891600541|     0|\n",
      "| 72|     1|     1|     1| 4.276666119016055|     0|\n",
      "| 83|     1|     1|     1| 4.418840607796598|     0|\n",
      "| 90|     1|     1|     1| 4.499809670330265|     0|\n",
      "+---+------+------+------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import log\n",
    "from pandas import DataFrame, Series, concat\n",
    "from numpy.random import default_rng, seed\n",
    "from numpy import select, full\n",
    "\n",
    "seed(117)\n",
    "\n",
    "N = 1000000\n",
    "\n",
    "features = DataFrame({\n",
    "  'a': default_rng().integers(10, 100, size=N),\n",
    "  'dummy1': full(shape=N, fill_value=1),\n",
    "  'dummy2': full(shape=N, fill_value=1),\n",
    "  'dummy3': full(shape=N, fill_value=1),\n",
    "})\n",
    "\n",
    "label = Series(log(features['a'].values), name='label')\n",
    "censor = Series(select(\n",
    " [\n",
    "  label > 5 ,\n",
    "  label <=5\n",
    " ],\n",
    " [1, 0] \n",
    "), name='censor')\n",
    "\n",
    "data = sc.createDataFrame(concat([features, label, censor], axis=1))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/09 13:06:41 WARN TaskSetManager: Stage 1173 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(a=20, dummy1=1, dummy2=1, dummy3=1, label=2.995732273553991, censor=0, features=DenseVector([20.0, 1.0, 1.0, 1.0]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import feature\n",
    "\n",
    "va = feature.VectorAssembler(inputCols=['a', 'dummy1', 'dummy2', 'dummy3'], outputCol='features')\n",
    "data_vec = va.transform(data)\n",
    "data_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/09 13:06:43 WARN TaskSetManager: Stage 1174 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:43 WARN TaskSetManager: Stage 1176 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1178 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1180 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1182 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1184 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1186 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1188 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1190 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1192 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1194 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1196 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1198 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1200 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1202 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1204 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1206 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1208 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1210 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1212 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1214 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1216 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:44 WARN TaskSetManager: Stage 1218 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:45 WARN TaskSetManager: Stage 1220 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:45 WARN TaskSetManager: Stage 1222 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:45 WARN TaskSetManager: Stage 1224 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:45 WARN TaskSetManager: Stage 1226 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:45 WARN TaskSetManager: Stage 1228 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/06/09 13:06:45 WARN TaskSetManager: Stage 1230 contains a task of very large size (1788 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "\n",
    "aft = AFTSurvivalRegression(\n",
    "  labelCol='label',\n",
    "  censorCol='censor',\n",
    "  featuresCol='features'\n",
    ")\n",
    "\n",
    "model = aft.fit(data_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0113, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
