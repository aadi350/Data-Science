{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/14 12:09:37 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.237 instead (on interface wlp5s0)\n",
      "23/06/14 12:09:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aadi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aadi/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0b38177c-336b-4cab-a721-7b0f9f2726d1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 94ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0b38177c-336b-4cab-a721-7b0f9f2726d1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/14 12:09:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/14 12:09:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder.appName('delta-tutorial').config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension').config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('year_month', 'timestamp'),\n",
       " ('num1', 'double'),\n",
       " ('num2', 'double'),\n",
       " ('num3', 'double')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, Series, date_range\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "\n",
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1073195249773562\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "## with looping\n",
    "A = 360\n",
    "B = 480\n",
    "sdf = spark.createDataFrame(DataFrame({\n",
    "  'id': ['a'] * A + ['b'] * B, \n",
    "  'year_month': list(date_range(start='2021-01-01', periods=A, freq='m')) + list(date_range(start='2020-01-01', periods=B, freq='m')),\n",
    "  'num1': random.normal(loc=1000, scale=200, size=A + B),\n",
    "  'num2': random.normal(loc=1000, scale=200, size= A + B),\n",
    "  'num3': random.normal(loc=1000, scale=200, size=A + B),\n",
    "}))\n",
    "\n",
    "window = (\n",
    "    Window\n",
    "    .partitionBy('id')\n",
    "    .orderBy('year_month')\n",
    "    .rowsBetween(-2, 0)\n",
    ")\n",
    "\n",
    "for w in [3, 6, 12]:\n",
    "  window = (\n",
    "      Window\n",
    "      .partitionBy('id')\n",
    "      .orderBy('year_month')\n",
    "      .rowsBetween(-w, 0)\n",
    "  )\n",
    "  for col in ['num1', 'num2','num3']:\n",
    "    sdf = sdf.withColumn('num1_window', F.avg(col).over(window))\n",
    "\n",
    "start = time.perf_counter()\n",
    "sdf.collect()\n",
    "\n",
    "print(time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12233384099090472\n"
     ]
    }
   ],
   "source": [
    "# without looping\n",
    "A = 360\n",
    "B = 480\n",
    "sdf = spark.createDataFrame(DataFrame({\n",
    "  'id': ['a'] * A + ['b'] * B, \n",
    "  'year_month': list(date_range(start='2021-01-01', periods=A, freq='m')) + list(date_range(start='2020-01-01', periods=B, freq='m')),\n",
    "  'num1': random.normal(loc=1000, scale=200, size=A + B),\n",
    "  'num2': random.normal(loc=1000, scale=200, size= A + B),\n",
    "  'num3': random.normal(loc=1000, scale=200, size=A + B),\n",
    "}))\n",
    "\n",
    "\n",
    "window = (\n",
    "    Window\n",
    "    .partitionBy('id')\n",
    "    .orderBy('year_month')\n",
    "    .rowsBetween(-2, 0)\n",
    ")\n",
    "\n",
    "\n",
    "sdf = sdf.select([\"*\"] + [F.avg(f\"{col}\").over(\n",
    "  Window.partitionBy('id')\n",
    "  .orderBy('year_month')\n",
    "  .rowsBetween(-w+1, 0)\n",
    ").alias(f'{col}_window') for col in ['num1', 'num2', 'num3'] for w in [3, 6, 9]]) # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "sdf.collect()\n",
    "print(time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "| id|         year_month|              num1|              num2|              num3|       num1_window|       num2_window|       num3_window|\n",
      "+---+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  a|2021-01-31 00:00:00| 838.0094799034778|1070.5663099934297|1155.3446824718862| 838.0094799034778|1070.5663099934297|1155.3446824718862|\n",
      "|  a|2021-02-28 00:00:00| 902.7514364697699| 721.9839584217593|1162.8678481010104| 870.3804581866239| 896.2751342075945|1159.1062652864484|\n",
      "|  a|2021-03-31 00:00:00| 941.4787028865572|1416.0157835140012| 974.5254693643411| 894.0798730866017|1069.5220173097302|1097.5793333124127|\n",
      "|  a|2021-04-30 00:00:00| 992.1368768305997| 1136.601558323544|1142.9520545473276|  945.455672062309|1091.5337667531014|1093.4484573375596|\n",
      "|  a|2021-05-31 00:00:00| 997.4781133067661|1077.5347159307441|1300.8200365551122| 977.0312310079744|1210.0506859227633|1139.4325201555937|\n",
      "|  a|2021-06-30 00:00:00|1222.8421085437617| 898.8464887019669| 812.0278913219656|1070.8190328937092|1037.6609209854182|1085.2666608081352|\n",
      "|  a|2021-07-31 00:00:00| 896.7836801292007| 1425.789755879811| 997.9779666528385|1039.0346339932428|1134.0569868375076|1036.9419648433054|\n",
      "|  a|2021-08-31 00:00:00|1038.2452576305263| 823.2898224708333|1046.1887000345055|1052.6236821011628| 1049.308689017537|   952.06485266977|\n",
      "|  a|2021-09-30 00:00:00| 940.3971739078179|1245.7589332202572| 1134.038939291269| 958.4753705558483|1164.9461705236338|1059.4018686595375|\n",
      "|  a|2021-10-31 00:00:00| 861.7817987387856|1158.9954320725726|1015.2116925035987| 946.8080767590433|1076.0147292545544|1065.1464439431245|\n",
      "|  a|2021-11-30 00:00:00|1147.9406044773584|  649.930895664674|  783.802911916959|  983.373192374654| 1018.228420319168| 977.6845145706088|\n",
      "|  a|2021-12-31 00:00:00| 758.5588861455909|1394.3633292462655| 839.6866618607617| 922.7604297872449| 1067.763218994504| 879.5670887604398|\n",
      "|  a|2022-01-31 00:00:00| 1295.199859971287| 635.5719980307633|1001.7502525331937|1067.2331168647454| 893.2887409805677| 875.0799421036381|\n",
      "|  a|2022-02-28 00:00:00| 906.7723686949823| 988.0274836413568|1472.5095105753783| 986.8437049372866|1005.9876036394618|1104.6488083231113|\n",
      "|  a|2022-03-31 00:00:00| 1252.537305703234|1064.4907633975859| 839.8395047159971|1151.5031781231676|  896.030081689902|1104.6997559415229|\n",
      "|  a|2022-04-30 00:00:00|1162.5312484126957| 865.6947903973463|1188.0665807125436|1107.2803076036373| 972.7376791454298| 1166.805198667973|\n",
      "|  a|2022-05-31 00:00:00|1071.9927127610522| 667.6407736059784|1306.7005598862465|1162.3537556256608| 865.9421091336368|1111.5355484382624|\n",
      "|  a|2022-06-30 00:00:00| 977.3805039297602|1316.5162893103306| 981.7686705092094|1070.6348217011694| 949.9506177712184| 1158.845270369333|\n",
      "|  a|2022-07-31 00:00:00|1108.5395470436226| 628.7234104775242|1197.9702192562145|1052.6375879114782| 870.9601577979444|1162.1464832172235|\n",
      "|  a|2022-08-31 00:00:00| 553.5023199318198|   969.84505197125| 918.5372582459377| 879.8074569684009|  971.694917253035|1032.7587160037872|\n",
      "+---+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
