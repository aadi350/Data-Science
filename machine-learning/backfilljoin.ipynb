{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.213:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff0ff24ddf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-------+\n",
      "| id|year|month|balance|\n",
      "+---+----+-----+-------+\n",
      "|  a|2022|    6|     10|\n",
      "|  a|2023|    3|     20|\n",
      "|  b|2022|    1|      0|\n",
      "|  b|2022|    2|      0|\n",
      "+---+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create test data\n",
    "from pandas import DataFrame\n",
    "df = DataFrame({\n",
    "    'id': ['a'] * 6 + ['b'] * 7, \n",
    "    'year': [2022, 2022, 2022, 2023, 2023, 2023, 2020, 2021, 2021, 2021, 2021, 2021, 2021],\n",
    "    'month': [10, 11, 12, 1, 2, 3, 12, 1, 4, 5, 8, 9, 12], \n",
    "    'balance': (round(i, 0) for i in np.random.normal(1000, 10, size=13))\n",
    "})\n",
    "\n",
    "\n",
    "df = DataFrame({\n",
    "    'id': ['a', 'a', 'b', 'b'],\n",
    "    'year': [2022, 2023, 2022, 2022],\n",
    "    'month': [6, 3, 1, 2],\n",
    "    'balance': [10, 20, 0, 0]\n",
    "})\n",
    "table = spark.createDataFrame(df)\n",
    "table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-------+----------+\n",
      "| id|year|month|balance|year_month|\n",
      "+---+----+-----+-------+----------+\n",
      "|  a|2022|    6|     10|2022-06-01|\n",
      "|  a|2023|    3|     20|2023-03-01|\n",
      "|  b|2022|    1|      0|2022-01-01|\n",
      "|  b|2022|    2|      0|2022-02-01|\n",
      "+---+----+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = table.withColumn('year_month', F.expr('make_date(year, month, 1)'))\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----+-------+\n",
      "| id|year_month|year|month|balance|\n",
      "+---+----------+----+-----+-------+\n",
      "|  a|2022-10-01|2022|   10|  997.0|\n",
      "|  a|2022-10-31|null| null|   null|\n",
      "|  a|2022-12-01|2022|   12| 1000.0|\n",
      "|  a|2022-12-31|null| null|   null|\n",
      "|  a|2023-01-31|null| null|   null|\n",
      "|  a|2023-03-01|2023|    3| 1013.0|\n",
      "|  b|2020-12-01|2020|   12| 1002.0|\n",
      "|  b|2020-12-31|null| null|   null|\n",
      "|  b|2021-01-31|null| null|   null|\n",
      "|  b|2021-03-01|null| null|   null|\n",
      "|  b|2021-03-31|null| null|   null|\n",
      "|  b|2021-05-01|2021|    5|  982.0|\n",
      "|  b|2021-05-31|null| null|   null|\n",
      "|  b|2021-07-01|null| null|   null|\n",
      "|  b|2021-07-31|null| null|   null|\n",
      "|  b|2021-08-31|null| null|   null|\n",
      "|  b|2021-10-01|null| null|   null|\n",
      "|  b|2021-10-31|null| null|   null|\n",
      "|  b|2021-12-01|2021|   12|  990.0|\n",
      "+---+----------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_dates_df.join(table, [ID_COL, DATE_COL], 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from datetime import datetime\n",
    "from numpy import datetime64\n",
    "\n",
    "class BackFillJoin:\n",
    "    '''Class for joining and filling time-series data\n",
    "\n",
    "    Class accepts a pyspark table and creates a consistent spine of year-month entries per-id\n",
    "    ending on either the maximum date per-id OR the ref-date (if specified)\n",
    "\n",
    "    Args:\n",
    "        id_col: identifies unique entities \n",
    "        date_col: string to identify column used as date, must of DateType\n",
    "        year_month_date: if set to true, transform() expects table with two columns named \"year\", and \"date\"\n",
    "        backfill: if set to true, returned data will be backfilled with the last non-null value\n",
    "        ref_date: if specified, time-series will be created up to this date point per-id\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if date_col is specified and year_month_date is not False\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, \n",
    "        id_col:str,\n",
    "        date_col: str = None,\n",
    "        year_month_date: bool = False,\n",
    "        backfill:bool= False, \n",
    "        ref_date: Optional[Union[str, datetime, datetime64]] = None\n",
    "    ):\n",
    "\n",
    "        if year_month_date and date_col is not None:\n",
    "            raise ValueError('if year_month date is specified, cannot specify separate date column, as dataframe is expected to have a column named \"year\" and \"month\"')\n",
    "\n",
    "        if ref_date and type(ref_date) not in (datetime64, datetime):\n",
    "            raise ValueError('ref_date must be np.datetime64 object or python datetime object')\n",
    "\n",
    "        self.id_col = id_col\n",
    "        self.backfill = backfill\n",
    "        self.ref_date = BackFillJoin._convert_ref_date(ref_date)\n",
    "        self.date_col = date_col\n",
    "        self.year_month_date = year_month_date\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_ref_date(ref_date):\n",
    "        if type(ref_date) is np.datetime64:\n",
    "            return np.datetime_as_string(ref_date, unit='D')\n",
    "        if type(ref_date) is datetime:\n",
    "            return ref_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "\n",
    "    def _create_year_month(self, table):\n",
    "        if self.year_month_date:\n",
    "            expr = F.expr('make_date(year, month, 1)')\n",
    "        else:\n",
    "            expr = F.expr(f'make_date(year({self.date_col}), month({self.date_col}), 1)')\n",
    "\n",
    "        return table.withColumn('year_month', expr)\n",
    "\n",
    "    def _create_all_dates(self, table):\n",
    "        grouped = table.groupBy(self.id_col).agg(\n",
    "                        F.max('year_month').alias(\"max_date\"),\n",
    "                        F.min('year_month').alias(\"min_date\"))\n",
    "\n",
    "        grouped.show()\n",
    "        if self.ref_date:\n",
    "            all_dates = (grouped\n",
    "                            .withColumn('ref_date', F.lit(self.ref_date))\n",
    "                            .select(\n",
    "                                self.id_col, \n",
    "                                F.expr(f\"sequence(to_date(min_date), to_date(ref_date), interval 1 month)\").alias('year_month'))\n",
    "                            .withColumn('year_month', F.explode('year_month')))\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_dates = (grouped\n",
    "                            .select(\n",
    "                                self.id_col, \n",
    "                                F.expr(\"sequence(to_date(min_date), to_date(max_date), interval 1 month)\").alias('year_month'))\n",
    "                            .withColumn('year_month', F.explode('year_month')))\n",
    "            \n",
    "        return all_dates\n",
    "\n",
    "    def transform(self, table):\n",
    "        table = self._create_year_month(table)\n",
    "        all_dates = self._create_all_dates(table)\n",
    "\n",
    "        all_dates.show(truncate=False)\n",
    "\n",
    "        if self.backfill:\n",
    "            w = Window.partitionBy(self.id_col).orderBy('year_month')\n",
    "            return (all_dates\n",
    "                .join(table, [self.id_col, 'year_month'], \"left\")\n",
    "                .select(self.id_col, 'year_month', *[F.last(F.col(c), ignorenulls=True).over(w).alias(c) for c in df.columns if c not in (self.id_col, 'year_month')] )\n",
    "                .withColumn('year', F.expr('year(year_month)'))\n",
    "                .withColumn('month', F.expr('month(year_month)')))\n",
    "\n",
    "        return (all_dates\n",
    "                .join(table, [self.id_col, 'year_month'], 'left')\n",
    "                .withColumn('year', F.expr('year(year_month)'))\n",
    "                .withColumn('month', F.expr('month(year_month)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "| id|balance|year_month|\n",
      "+---+-------+----------+\n",
      "|  a|     10|2022-06-01|\n",
      "|  a|     20|2023-03-01|\n",
      "|  b|      0|2022-01-01|\n",
      "|  b|      0|2022-02-01|\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "s = BackFillJoin(id_col='id',date_col='year_month', backfill=True)\n",
    "table.drop(*['year', 'month']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|  max_date|  min_date|\n",
      "+---+----------+----------+\n",
      "|  a|2023-03-01|2022-06-01|\n",
      "|  b|2022-02-01|2022-01-01|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+\n",
      "|id |year_month|\n",
      "+---+----------+\n",
      "|a  |2022-06-01|\n",
      "|a  |2022-07-01|\n",
      "|a  |2022-08-01|\n",
      "|a  |2022-09-01|\n",
      "|a  |2022-10-01|\n",
      "|a  |2022-11-01|\n",
      "|a  |2022-12-01|\n",
      "|a  |2023-01-01|\n",
      "|a  |2023-02-01|\n",
      "|a  |2023-03-01|\n",
      "|b  |2022-01-01|\n",
      "|b  |2022-02-01|\n",
      "+---+----------+\n",
      "\n",
      "+---+----------+----+-----+-------+\n",
      "| id|year_month|year|month|balance|\n",
      "+---+----------+----+-----+-------+\n",
      "|  a|2022-06-01|2022|    6|     10|\n",
      "|  a|2022-07-01|2022|    7|     10|\n",
      "|  a|2022-08-01|2022|    8|     10|\n",
      "|  a|2022-09-01|2022|    9|     10|\n",
      "|  a|2022-10-01|2022|   10|     10|\n",
      "|  a|2022-11-01|2022|   11|     10|\n",
      "|  a|2022-12-01|2022|   12|     10|\n",
      "|  a|2023-01-01|2023|    1|     10|\n",
      "|  a|2023-02-01|2023|    2|     10|\n",
      "|  a|2023-03-01|2023|    3|     20|\n",
      "|  b|2022-01-01|2022|    1|      0|\n",
      "|  b|2022-02-01|2022|    2|      0|\n",
      "+---+----------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.transform(table).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-01-01'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('spark_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
