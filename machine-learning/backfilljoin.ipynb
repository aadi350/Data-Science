{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/bin/spark-class: line 111: /usr/lib/jvm/java-8-openjdk-amd-64/bin/java: No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m functions \u001b[39mas\u001b[39;00m F\n\u001b[0;32m----> 6\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[1;32m      8\u001b[0m spark\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 339\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    340\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    111\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-------+\n",
      "| id|year|month|balance|\n",
      "+---+----+-----+-------+\n",
      "|  a|2022|    6|     10|\n",
      "|  a|2023|    3|     20|\n",
      "|  b|2022|    1|      0|\n",
      "|  b|2022|    2|      0|\n",
      "+---+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create test data\n",
    "from pandas import DataFrame\n",
    "df = DataFrame({\n",
    "    'id': ['a'] * 6 + ['b'] * 7, \n",
    "    'year': [2022, 2022, 2022, 2023, 2023, 2023, 2020, 2021, 2021, 2021, 2021, 2021, 2021],\n",
    "    'month': [10, 11, 12, 1, 2, 3, 12, 1, 4, 5, 8, 9, 12], \n",
    "    'balance': (round(i, 0) for i in np.random.normal(1000, 10, size=13))\n",
    "})\n",
    "\n",
    "\n",
    "df = DataFrame({\n",
    "    'id': ['a', 'a', 'b', 'b'],\n",
    "    'year': [2022, 2023, 2022, 2022],\n",
    "    'month': [6, 3, 1, 2],\n",
    "    'balance': [10, 20, 0, 0]\n",
    "})\n",
    "table = spark.createDataFrame(df)\n",
    "table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-------+----------+\n",
      "| id|year|month|balance|year_month|\n",
      "+---+----+-----+-------+----------+\n",
      "|  a|2022|    6|     10|2022-06-01|\n",
      "|  a|2023|    3|     20|2023-03-01|\n",
      "|  b|2022|    1|      0|2022-01-01|\n",
      "|  b|2022|    2|      0|2022-02-01|\n",
      "+---+----+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = table.withColumn('year_month', F.expr('make_date(year, month, 1)'))\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----+-------+\n",
      "| id|year_month|year|month|balance|\n",
      "+---+----------+----+-----+-------+\n",
      "|  a|2022-10-01|2022|   10|  997.0|\n",
      "|  a|2022-10-31|null| null|   null|\n",
      "|  a|2022-12-01|2022|   12| 1000.0|\n",
      "|  a|2022-12-31|null| null|   null|\n",
      "|  a|2023-01-31|null| null|   null|\n",
      "|  a|2023-03-01|2023|    3| 1013.0|\n",
      "|  b|2020-12-01|2020|   12| 1002.0|\n",
      "|  b|2020-12-31|null| null|   null|\n",
      "|  b|2021-01-31|null| null|   null|\n",
      "|  b|2021-03-01|null| null|   null|\n",
      "|  b|2021-03-31|null| null|   null|\n",
      "|  b|2021-05-01|2021|    5|  982.0|\n",
      "|  b|2021-05-31|null| null|   null|\n",
      "|  b|2021-07-01|null| null|   null|\n",
      "|  b|2021-07-31|null| null|   null|\n",
      "|  b|2021-08-31|null| null|   null|\n",
      "|  b|2021-10-01|null| null|   null|\n",
      "|  b|2021-10-31|null| null|   null|\n",
      "|  b|2021-12-01|2021|   12|  990.0|\n",
      "+---+----------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_dates_df.join(table, [ID_COL, DATE_COL], 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from datetime import datetime\n",
    "from numpy import datetime64\n",
    "\n",
    "class BackFillJoin:\n",
    "    '''Class for joining and filling time-series data\n",
    "\n",
    "    Class accepts a pyspark table and creates a consistent spine of year-month entries per-id\n",
    "    ending on either the maximum date per-id OR the ref-date (if specified)\n",
    "\n",
    "    Args:\n",
    "        id_col: identifies unique entities \n",
    "        date_col: string to identify column used as date, must of DateType\n",
    "        year_month_date: if set to true, transform() expects table with two columns named \"year\", and \"date\"\n",
    "        backfill: if set to true, returned data will be backfilled with the last non-null value\n",
    "        ref_date: if specified, time-series will be created up to this date point per-id\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if date_col is specified and year_month_date is not False\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, \n",
    "        id_col:str,\n",
    "        date_col: str = None,\n",
    "        year_month_date: bool = False,\n",
    "        backfill:bool= False, \n",
    "        ref_date: Optional[Union[str, datetime, datetime64]] = None\n",
    "    ):\n",
    "\n",
    "        if year_month_date and date_col is not None:\n",
    "            raise ValueError('if year_month date is specified, cannot specify separate date column, as dataframe is expected to have a column named \"year\" and \"month\"')\n",
    "\n",
    "        if ref_date and type(ref_date) not in (datetime64, datetime):\n",
    "            raise ValueError('ref_date must be np.datetime64 object or python datetime object')\n",
    "\n",
    "        self.id_col = id_col\n",
    "        self.backfill = backfill\n",
    "        self.ref_date = BackFillJoin._convert_ref_date(ref_date)\n",
    "        self.date_col = date_col\n",
    "        self.year_month_date = year_month_date\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_ref_date(ref_date):\n",
    "        if type(ref_date) is np.datetime64:\n",
    "            return np.datetime_as_string(ref_date, unit='D')\n",
    "        if type(ref_date) is datetime:\n",
    "            return ref_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "\n",
    "    def _create_year_month(self, table):\n",
    "        if self.year_month_date:\n",
    "            expr = F.expr('make_date(year, month, 1)')\n",
    "        else:\n",
    "            expr = F.expr(f'make_date(year({self.date_col}), month({self.date_col}), 1)')\n",
    "\n",
    "        return table.withColumn('year_month', expr)\n",
    "\n",
    "    def _create_all_dates(self, table):\n",
    "        grouped = table.groupBy(self.id_col).agg(\n",
    "                        F.max('year_month').alias(\"max_date\"),\n",
    "                        F.min('year_month').alias(\"min_date\"))\n",
    "\n",
    "        grouped.show()\n",
    "        if self.ref_date:\n",
    "            all_dates = (grouped\n",
    "                            .withColumn('ref_date', F.lit(self.ref_date))\n",
    "                            .select(\n",
    "                                self.id_col, \n",
    "                                F.expr(f\"sequence(to_date(min_date), to_date(ref_date), interval 1 month)\").alias('year_month'))\n",
    "                            .withColumn('year_month', F.explode('year_month')))\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_dates = (grouped\n",
    "                            .select(\n",
    "                                self.id_col, \n",
    "                                F.expr(\"sequence(to_date(min_date), to_date(max_date), interval 1 month)\").alias('year_month'))\n",
    "                            .withColumn('year_month', F.explode('year_month')))\n",
    "            \n",
    "        return all_dates\n",
    "\n",
    "    def transform(self, table):\n",
    "        table = self._create_year_month(table)\n",
    "        all_dates = self._create_all_dates(table)\n",
    "\n",
    "        all_dates.show(truncate=False)\n",
    "\n",
    "        if self.backfill:\n",
    "            w = Window.partitionBy(self.id_col).orderBy('year_month')\n",
    "            return (all_dates\n",
    "                .join(table, [self.id_col, 'year_month'], \"left\")\n",
    "                .select(self.id_col, 'year_month', *[F.last(F.col(c), ignorenulls=True).over(w).alias(c) for c in df.columns if c not in (self.id_col, 'year_month')] )\n",
    "                .withColumn('year', F.expr('year(year_month)'))\n",
    "                .withColumn('month', F.expr('month(year_month)')))\n",
    "\n",
    "        return (all_dates\n",
    "                .join(table, [self.id_col, 'year_month'], 'left')\n",
    "                .withColumn('year', F.expr('year(year_month)'))\n",
    "                .withColumn('month', F.expr('month(year_month)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "| id|balance|year_month|\n",
      "+---+-------+----------+\n",
      "|  a|     10|2022-06-01|\n",
      "|  a|     20|2023-03-01|\n",
      "|  b|      0|2022-01-01|\n",
      "|  b|      0|2022-02-01|\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "s = BackFillJoin(id_col='id',date_col='year_month', backfill=True)\n",
    "table.drop(*['year', 'month']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|  max_date|  min_date|\n",
      "+---+----------+----------+\n",
      "|  a|2023-03-01|2022-06-01|\n",
      "|  b|2022-02-01|2022-01-01|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+\n",
      "|id |year_month|\n",
      "+---+----------+\n",
      "|a  |2022-06-01|\n",
      "|a  |2022-07-01|\n",
      "|a  |2022-08-01|\n",
      "|a  |2022-09-01|\n",
      "|a  |2022-10-01|\n",
      "|a  |2022-11-01|\n",
      "|a  |2022-12-01|\n",
      "|a  |2023-01-01|\n",
      "|a  |2023-02-01|\n",
      "|a  |2023-03-01|\n",
      "|b  |2022-01-01|\n",
      "|b  |2022-02-01|\n",
      "+---+----------+\n",
      "\n",
      "+---+----------+----+-----+-------+\n",
      "| id|year_month|year|month|balance|\n",
      "+---+----------+----+-----+-------+\n",
      "|  a|2022-06-01|2022|    6|     10|\n",
      "|  a|2022-07-01|2022|    7|     10|\n",
      "|  a|2022-08-01|2022|    8|     10|\n",
      "|  a|2022-09-01|2022|    9|     10|\n",
      "|  a|2022-10-01|2022|   10|     10|\n",
      "|  a|2022-11-01|2022|   11|     10|\n",
      "|  a|2022-12-01|2022|   12|     10|\n",
      "|  a|2023-01-01|2023|    1|     10|\n",
      "|  a|2023-02-01|2023|    2|     10|\n",
      "|  a|2023-03-01|2023|    3|     20|\n",
      "|  b|2022-01-01|2022|    1|      0|\n",
      "|  b|2022-02-01|2022|    2|      0|\n",
      "+---+----------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.transform(table).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-01-01'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('spark_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
