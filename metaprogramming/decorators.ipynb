{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-1.8.0-openjdk-amd64'\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/aadi/miniconda3/envs/pyspark_env/bin/python' \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/aadi/miniconda3/envs/pyspark_env/bin/python' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorators\n",
    "*Why use decorators instead of (e.g.) asserts*\n",
    "1. Clutters functions with error-checking logic\n",
    "2. If validation logic needs to change, many inline copies need to be found and updated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Decorator\n",
    "\n",
    "```python\n",
    "def decorator(input_fn):\n",
    "    def _decorate(*args, **kwargs):\n",
    "        print('decorating')\n",
    "\n",
    "        return input_fn(*args, **kwargs)\n",
    "\n",
    "    return _decorate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fndecorator(input_fn):\n",
    "    def decorator():\n",
    "        print('This is from decorator')\n",
    "\n",
    "        return input_fn()\n",
    "\n",
    "    return decorator \n",
    "\n",
    "\n",
    "@fndecorator\n",
    "def new_fn():\n",
    "    print('from original function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is from decorator\n",
      "from original function\n"
     ]
    }
   ],
   "source": [
    "new_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manager_albany(*args):\n",
    "    BLUE = '\\033[94m'\n",
    "    BOLD = '\\33[5m'\n",
    "    SELECT = '\\33[7m'\n",
    "    for arg in args:\n",
    "        print(BLUE + BOLD + SELECT + str(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_with_input(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "\n",
    "def add_line_function(function_with_input):\n",
    "    def add_line(*args):\n",
    "        print('ADDED LINE')\n",
    "        return function_with_input(*args)\n",
    "    return add_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_line_function\n",
    "def fn(*args):\n",
    "    for arg in args:\n",
    "        print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDED LINE\n",
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "fn('a', 'b', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator(input_fn):\n",
    "    def _decorate(*args, **kwargs):\n",
    "        print('decorating')\n",
    "\n",
    "        return input_fn(*args, **kwargs)\n",
    "\n",
    "    return _decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def fn(input_arg):\n",
    "    print(input_arg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datefixer(fn):\n",
    "    import datetime\n",
    "    def decorator(*args):\n",
    "        newargs = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg, datetime.date):\n",
    "                arg = arg.weekday(), arg.day, arg.month, arg.year\n",
    "            newargs.append(arg)\n",
    "        return fn(*newargs)\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@datefixer\n",
    "def set_holidays(*args):\n",
    "    return args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 25, 0, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "some_date = dt.strptime('2022-12-25', '%Y-%m-%d')\n",
    "some_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 25, 12, 2022)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_holidays(some_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 18:16:03 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.213 instead (on interface wlp5s0)\n",
      "22/12/28 18:16:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 18:16:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>american</th>\n",
       "      <th>monthname</th>\n",
       "      <th>julian</th>\n",
       "      <th>inversejulian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06/07/2022</td>\n",
       "      <td>06/July/2022</td>\n",
       "      <td>1997/310</td>\n",
       "      <td>310/1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     american     monthname    julian inversejulian\n",
       "0  06/07/2022  06/July/2022  1997/310      310/1997"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder.appName('decorators').getOrCreate()\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'american': ['06/07/2022'],\n",
    "    'monthname': ['06/July/2022'],\n",
    "    'julian': ['1997/310'],\n",
    "    'inversejulian': ['310/1997'],\n",
    "\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadi/miniconda3/envs/pyspark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/aadi/miniconda3/envs/pyspark_env/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-------------+\n",
      "|  american|   monthname|  julian|inversejulian|\n",
      "+----------+------------+--------+-------------+\n",
      "|06/07/2022|06/July/2022|1997/310|     310/1997|\n",
      "+----------+------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sc.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m functions \u001b[39mas\u001b[39;00m F \n\u001b[0;32m----> 2\u001b[0m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mamerican\u001b[39m\u001b[39m'\u001b[39m, F\u001b[39m.\u001b[39mto_date(\u001b[39m'\u001b[39m\u001b[39mamerican\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdd/MM/yyyy\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "df.withColumn('american', F.to_date('american', 'dd/MM/yyyy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------------+\n",
      "|  american|   monthname|    julian|inversejulian|\n",
      "+----------+------------+----------+-------------+\n",
      "|06/07/2022|06/July/2022|1997-11-06|     310/1997|\n",
      "+----------+------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn('julian', F.to_date('julian', 'yyyy/DDD')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-------------+\n",
      "|  american|   monthname|  julian|inversejulian|\n",
      "+----------+------------+--------+-------------+\n",
      "|06/07/2022|06/July/2022|1997/310|   1997-11-06|\n",
      "+----------+------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn('inversejulian', F.to_date('inversejulian', 'DDD/yyyy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+\n",
      "|  american|monthname|  julian|inversejulian|\n",
      "+----------+---------+--------+-------------+\n",
      "|06/07/2022|     null|1997/310|     310/1997|\n",
      "+----------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('monthname', F.to_date('monthname', 'dd/LL/yyyy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "date_cols = {\n",
    "    'american': ['american', 'american1'],\n",
    "    'julian': ['julian']\n",
    "}\n",
    "\n",
    "\n",
    "def datefixer(dateconf):\n",
    "    import pyspark\n",
    "    def _datefixer(func):\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(df, *args, **kwargs):\n",
    "            df_dateconf = {}\n",
    "            for key, values in dateconf.items():\n",
    "                df_dateconf[key] = [i for i in df.columns if i in values]\n",
    "\n",
    "\n",
    "            for dateformat in df_dateconf.keys():\n",
    "                for datecolumn in df_dateconf[dateformat]:\n",
    "                    print('converting', dateformat)\n",
    "                    if dateformat == 'american':\n",
    "                        df = df.withColumn(datecolumn, F.to_date(datecolumn, 'dd/MM/yyyy'))\n",
    "                    if dateformat == 'julian':\n",
    "                        df = df.withColumn(datecolumn, F.to_date(datecolumn, 'yyyy/DDD'))\n",
    "            return func(df, *args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return _datefixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "@datefixer(dateconf=date_cols)\n",
    "def test(df):\n",
    "\n",
    "    return df \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+\n",
      "|  american|  julian|inversejulian|\n",
      "+----------+--------+-------------+\n",
      "|06/07/2022|1997/310|     310/1997|\n",
      "+----------+--------+-------------+\n",
      "\n",
      "converting american\n",
      "converting julian\n",
      "+----------+----------+-------------+\n",
      "|  american|    julian|inversejulian|\n",
      "+----------+----------+-------------+\n",
      "|2022-07-06|1997-11-06|     310/1997|\n",
      "+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "test(df=df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "def fn(*args, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, pyspark.sql.DataFrame):\n",
    "            kwargs[k] = kwargs[k].withColumn('new', F.lit(0))\n",
    "    return kwargs \n",
    "    print(kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datefixer(conf):\n",
    "    import pyspark\n",
    "    def _datefixer(func):\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for k, v in kwargs.items():\n",
    "                if isinstance(v, pyspark.sql.DataFrame):\n",
    "                    print('found a dataframe')\n",
    "                    kwargs[k] = kwargs[k].withColumn('col', F.lit(0))\n",
    "\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return _datefixer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "@datefixer(conf=None)\n",
    "def process(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found a dataframe\n",
      "+----------+--------+-------------+---+\n",
      "|  american|  julian|inversejulian|col|\n",
      "+----------+--------+-------------+---+\n",
      "|06/07/2022|1997/310|     310/1997|  0|\n",
      "+----------+--------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process(df=df).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators with Args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this \n",
    "```python\n",
    "@decorator(args)\n",
    "def func():\n",
    "    pass\n",
    "```\n",
    "\n",
    "is essentially this\n",
    "```python\n",
    "func = decorator(args)(func)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def debug(prefix=''): # this outer function provides an \"environment\" for the inner functions\n",
    "    def decorate(func):\n",
    "        msg = prefix + func.__qualname__\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            print(msg)\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "@debug('***')\n",
    "def foo(a=1):\n",
    "    return a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***foo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing repetition\n",
    "from functools import wraps, partial\n",
    "\n",
    "def debug(func=None, *, prefix=''):\n",
    "    if func is None:\n",
    "        return partial(debug, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fndecorator(input_fn):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    def decorator(*args, **kwargs):\n",
    "        result = input_fn(*args, **kwargs)\n",
    "\n",
    "        return result\n",
    "    return decorator \n",
    "\n",
    "\n",
    "def process(*args, **kwargs):\n",
    "    print(locals())\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = fndecorator(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hello(name):\n",
    "    return f'Hello {name}'\n",
    "\n",
    "\n",
    "def say_yo(name):\n",
    "    return f'Yo {name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(greeter):\n",
    "    return greeter('Bob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yo Bob'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greet(say_hello) \n",
    "greet(say_yo) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structure for slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hi, I am Emma', 'Call me Liam')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first-class objects\n",
    "# being able to pass in and return functions\n",
    "# passing it in two ways, with () calls the function, without is a reference\n",
    "def run_tf_model(data):\n",
    "    return f'tf model run on {data}' \n",
    "\n",
    "def run_torch_model(data):\n",
    "    return f'torch model run on {data}' \n",
    "\n",
    "def run_model_on_data(runner):\n",
    "    return runner(data=[1, 2, 3])\n",
    "\n",
    "run_model_on_data(run_torch_model), run_model_on_data(run_tf_model)\n",
    "\n",
    "# being able to return functions\n",
    "def parent(num):\n",
    "    def first_child():\n",
    "        return \"Hi, I am Emma\"\n",
    "\n",
    "    def second_child():\n",
    "        return \"Call me Liam\"\n",
    "\n",
    "    if num == 1:\n",
    "        return first_child\n",
    "    else:\n",
    "        return second_child\n",
    "\n",
    "first = parent(1)\n",
    "second = parent(2)\n",
    "\n",
    "first(), second()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "foo\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "# decorators are not inherently complex\n",
    "# they just take functions in, modify the behaviour in-place, and return the function\n",
    "# we define an inner function wrapper which adds functionality to f\n",
    "#   we can intercept the inputs and the outputs of the function\n",
    "#   and return it, so now, foo is not being called, but wrapper is!\n",
    "#   show with and without wraps\n",
    "\n",
    "def decorator(f):\n",
    "\n",
    "    # @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('within decorator')\n",
    "        result = f(*args, **kwargs)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def foo(*args, **kwargs):\n",
    "    print('foo')\n",
    "    return 0\n",
    "\n",
    "print(foo.__name__)\n",
    "foo = decorator(foo)\n",
    "print(foo.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator(conf):\n",
    "\n",
    "    def _wrapper(f):\n",
    "\n",
    "        @wraps(f)\n",
    "        def _wrapped(*args, **kwargs):\n",
    "\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return _wrapped\n",
    "\n",
    "    return _wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator(None)\n",
    "def baz(*args, **kwargs):\n",
    "    print('baz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baz\n"
     ]
    }
   ],
   "source": [
    "baz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pyspark_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:35) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c381e67c3557bc3722f390a24d9b4637bf98cf80995ba2adfe7961e7381da7d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
