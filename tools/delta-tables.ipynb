{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Hands-On Introduction to Delta Tables in Python \n",
    "\n",
    "Delta is a versioned parquet file with a transaction log, we get compression and all benefits of parquet  \n",
    "Transaction log is single-source of truth, see who does what, plays nicely with spark and python  \n",
    "\n",
    "\n",
    "Python APIs for Delta-Lake\n",
    "- `pyspark`\n",
    "- `delta-rs` pip install delta-lake\n",
    "- pyspark declarative `pip install delta-spark`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 08:57:56 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.237 instead (on interface wlp5s0)\n",
      "23/06/24 08:57:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aadi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aadi/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4c5d06b1-2358-45b9-9407-8a49dcb9bd07;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 111ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4c5d06b1-2358-45b9-9407-8a49dcb9bd07\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 08:57:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder.appName('delta-tutorial').config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 94.30% for 8 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 83.83% for 9 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 75.44% for 10 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 68.58% for 11 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 62.87% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/10 12:36:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 68.58% for 11 writers\n",
      "23/06/10 12:36:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 75.44% for 10 writers\n",
      "23/06/10 12:36:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 83.83% for 9 writers\n",
      "23/06/10 12:36:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 94.30% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load vanilla spark table\n",
    "sdf = spark.read.load(\n",
    "  '/storage/data/airline_2m.csv' ,\n",
    "  format='com.databricks.spark.csv',\n",
    "  header='true',\n",
    "  inferSchema='true'\n",
    ").select(['FlightDate', 'Reporting_Airline', 'Flight_Number_Reporting_Airline','Origin', 'Dest', 'DepTime', 'DepDelay', 'ArrTime', 'ArrDelay' ])\n",
    "\n",
    "# save as a delta table\n",
    "sdf.write.format('delta').mode('overwrite').save('/storage/data/airline_2m.delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, '/storage/data/airline_2m.delta/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00008-37835621-905f-481b-bee1-f2fbde02595c-c000.snappy.parquet',\n",
       " '.part-00002-f79474ae-503d-4914-8aff-54af17da6de2-c000.snappy.parquet.crc',\n",
       " '.part-00006-9cdf358f-5ff7-4c45-9314-09aeeb770ab4-c000.snappy.parquet.crc',\n",
       " '.part-00001-da20e657-cbd5-4ca0-b7bf-f862a6083d2b-c000.snappy.parquet.crc',\n",
       " 'part-00007-3e4232fc-fbd4-4cce-84fb-7a862aff9928-c000.snappy.parquet',\n",
       " '.part-00010-bff9f938-783e-490b-b462-636415a9f94d-c000.snappy.parquet.crc',\n",
       " '.part-00009-6c30192d-45cc-4819-b3ce-3e6c4de1c218-c000.snappy.parquet.crc',\n",
       " 'part-00004-016a4d0d-6026-4cff-9c53-8a4af7c32241-c000.snappy.parquet',\n",
       " 'part-00005-3885ce51-666c-4fb2-a0ad-a12c90ec095a-c000.snappy.parquet',\n",
       " '_delta_log',\n",
       " 'part-00003-9aec4e25-09dd-4f1d-8bff-e6c7c368ae7d-c000.snappy.parquet',\n",
       " '.part-00008-37835621-905f-481b-bee1-f2fbde02595c-c000.snappy.parquet.crc',\n",
       " 'part-00006-9cdf358f-5ff7-4c45-9314-09aeeb770ab4-c000.snappy.parquet',\n",
       " 'part-00009-6c30192d-45cc-4819-b3ce-3e6c4de1c218-c000.snappy.parquet',\n",
       " '.part-00003-9aec4e25-09dd-4f1d-8bff-e6c7c368ae7d-c000.snappy.parquet.crc',\n",
       " 'part-00002-f79474ae-503d-4914-8aff-54af17da6de2-c000.snappy.parquet',\n",
       " 'part-00010-bff9f938-783e-490b-b462-636415a9f94d-c000.snappy.parquet',\n",
       " 'part-00011-eeea5de3-5e91-47df-9762-7d673770f769-c000.snappy.parquet',\n",
       " '.part-00011-eeea5de3-5e91-47df-9762-7d673770f769-c000.snappy.parquet.crc',\n",
       " '.part-00007-3e4232fc-fbd4-4cce-84fb-7a862aff9928-c000.snappy.parquet.crc',\n",
       " '.part-00005-3885ce51-666c-4fb2-a0ad-a12c90ec095a-c000.snappy.parquet.crc',\n",
       " 'part-00001-da20e657-cbd5-4ca0-b7bf-f862a6083d2b-c000.snappy.parquet',\n",
       " '.part-00000-9642bf76-d935-42ae-bbe4-64d7b1c52df2-c000.snappy.parquet.crc',\n",
       " 'part-00000-9642bf76-d935-42ae-bbe4-64d7b1c52df2-c000.snappy.parquet',\n",
       " '.part-00004-016a4d0d-6026-4cff-9c53-8a4af7c32241-c000.snappy.parquet.crc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting what we just wrote\n",
    "import os\n",
    "\n",
    "os.listdir('/storage/data/airline_2m.delta/')\n",
    "# bunch of individual files and a folder called \"_delta_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00000000000000000000.json', '.00000000000000000000.json.crc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _delta_log\n",
    "os.listdir('/storage/data/airline_2m.delta/_delta_log/')\n",
    "# a json and a .crc file, crc files are checksums added to prevent corruption if parquet is corrupted in-flight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logs hold:\n",
    "copy-paste:\n",
    "Whenever a user performs an operation to modify a table (such as an INSERT, UPDATE or DELETE), Delta Lake breaks that operation down into a series of discrete steps composed of one or more of the actions below.\n",
    "\n",
    "Add file - adds a data file.\n",
    "Remove file - removes a data file.\n",
    "Update metadata - Updates the table’s metadata (e.g., changing the table’s name, schema or partitioning).\n",
    "Set transaction - Records that a structured streaming job has committed a micro-batch with the given ID.\n",
    "Change protocol - enables new features by switching the Delta Lake transaction log to the newest software protocol.\n",
    "Commit info - Contains information around the commit, which operation was made, from where and at what time.\n",
    "\n",
    "\n",
    "WHen table is created, table's transaciton log automatically created in `_delta_log`. Each change is recorded as an atomic commit in the transaction log\n",
    "\n",
    "Time-travel `DESCRIBE HISTORY` in SQL, can select fomr using `TIMESTAMP` OR the `VERSION` `TABLE@v2` \n",
    "\n",
    "\n",
    "Delta transaciton enables ACID, full audit and scalable metadata.\n",
    "\n",
    "Hive metastore stores table definition, where table is stored\n",
    "\n",
    "\n",
    "# CTAS Statements\n",
    "`CREATE_TABLE _ AS SELECT` use output of select to crae\n",
    "\n",
    "Does not support manual schema declaration, automatically infers schema from query results, does not require an `INSERT` statement.\n",
    "\n",
    "```CRETE TABLE new_table\n",
    "COMMENT \"some comment\"\n",
    "PARTITIONED BY (id1, id2) --best practice to non-partition\n",
    "LOCATION '/some/path'\n",
    "FROM ...\n",
    "```\n",
    "\n",
    "# Constraints\n",
    "- NOT NULL and CHECK supported\n",
    "`CHECK` looks like `WHERE` clauses, `NOT NULL` is obvious\n",
    "\n",
    "# Copying\n",
    "**DEEP** clone  \n",
    "`CREATE TABLE table_clone DEEP CLONE source_table` can occur incrementall (done multiple times)\n",
    "\n",
    "**SHALLOW** clone  \n",
    "only copies transaction log, no actual data copied  \n",
    "\n",
    "\n",
    "# Things to observe \n",
    "- Updates\n",
    "- Delete\n",
    "- Optimize: compacts multiple small files\n",
    "- ZORDER BY: added to optimize, this compacts files ordered by the column, like indexing\n",
    "    - Zordering for high-cardinality columns (>= 2 columns): can I go ahead and skip (i.e. reduce number of files need to scan), diminishing returns if z-order by all columns\n",
    "    - Partitioning is for low-cardinality columns, table >= 1TB of data, partitions >= 1GB\n",
    "- Cleaning up: delta lake `VACUUM table_name [retention period]`, feault retention period is 7 days. Once vacuum is run, time-travel is lost\n",
    "- Try deleting then using time-travel to go back using `RESTORE TABLE` then calling `DESCRIBE HISTORY`  \n",
    "\n",
    "can call `delta_table.history().show()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\": {\n",
      "    \"timestamp\": 1686413205823,\n",
      "    \"operation\": \"WRITE\",\n",
      "    \"operationParameters\": {\n",
      "      \"mode\": \"Overwrite\",\n",
      "      \"partitionBy\": \"[]\"\n",
      "    },\n",
      "    \"isolationLevel\": \"Serializable\",\n",
      "    \"isBlindAppend\": false,\n",
      "    \"operationMetrics\": {\n",
      "      \"numFiles\": \"12\",\n",
      "      \"numOutputRows\": \"2000000\",\n",
      "      \"numOutputBytes\": \"20619620\"\n",
      "    },\n",
      "    \"engineInfo\": \"Apache-Spark/3.3.2 Delta-Lake/2.3.0\",\n",
      "    \"txnId\": \"4f5656f9-d9d2-404e-bf11-722a1349387b\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"protocol\": {\n",
      "    \"minReaderVersion\": 1,\n",
      "    \"minWriterVersion\": 2\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"metaData\": {\n",
      "    \"id\": \"609a6c9c-5c5d-4844-9276-49201985b8f1\",\n",
      "    \"format\": {\n",
      "      \"provider\": \"parquet\",\n",
      "      \"options\": {}\n",
      "    },\n",
      "    \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"FlightDate\\\",\\\"type\\\":\\\"timestamp\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"Reporting_Airline\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"Origin\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"Dest\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"DepTime\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"DepDelay\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"ArrTime\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"ArrDelay\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "    \"partitionColumns\": [],\n",
      "    \"configuration\": {},\n",
      "    \"createdTime\": 1686413204081\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00000-9642bf76-d935-42ae-bbe4-64d7b1c52df2-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726172,\n",
      "    \"modificationTime\": 1686413205702,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167450,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-60.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-89.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1153.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3012,\\\"DepDelay\\\":3016,\\\"ArrTime\\\":3309,\\\"ArrDelay\\\":3446}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00001-da20e657-cbd5-4ca0-b7bf-f862a6083d2b-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726016,\n",
      "    \"modificationTime\": 1686413205734,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167446,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-82.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-80.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1434.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1295.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2997,\\\"DepDelay\\\":3005,\\\"ArrTime\\\":3307,\\\"ArrDelay\\\":3436}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00002-f79474ae-503d-4914-8aff-54af17da6de2-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1725906,\n",
      "    \"modificationTime\": 1686413205706,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167449,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-62.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-90.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1855.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1847.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2998,\\\"DepDelay\\\":3009,\\\"ArrTime\\\":3304,\\\"ArrDelay\\\":3437}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00003-9aec4e25-09dd-4f1d-8bff-e6c7c368ae7d-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726098,\n",
      "    \"modificationTime\": 1686413205622,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167467,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-67.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-81.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1878.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1898.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2989,\\\"DepDelay\\\":2994,\\\"ArrTime\\\":3252,\\\"ArrDelay\\\":3388}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00004-016a4d0d-6026-4cff-9c53-8a4af7c32241-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726276,\n",
      "    \"modificationTime\": 1686413205718,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167462,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-60.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-77.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1628.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1631.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3042,\\\"DepDelay\\\":3044,\\\"ArrTime\\\":3325,\\\"ArrDelay\\\":3474}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00005-3885ce51-666c-4fb2-a0ad-a12c90ec095a-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1727251,\n",
      "    \"modificationTime\": 1686413205710,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167481,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-49.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-90.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1458.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3059,\\\"DepDelay\\\":3066,\\\"ArrTime\\\":3380,\\\"ArrDelay\\\":3519}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00006-9cdf358f-5ff7-4c45-9314-09aeeb770ab4-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726159,\n",
      "    \"modificationTime\": 1686413205670,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167477,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-62.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-76.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1430.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1402.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3069,\\\"DepDelay\\\":3073,\\\"ArrTime\\\":3357,\\\"ArrDelay\\\":3470}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00007-3e4232fc-fbd4-4cce-84fb-7a862aff9928-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1725890,\n",
      "    \"modificationTime\": 1686413205702,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167445,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-60.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-91.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1538.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1532.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2989,\\\"DepDelay\\\":2994,\\\"ArrTime\\\":3301,\\\"ArrDelay\\\":3447}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00008-37835621-905f-481b-bee1-f2fbde02595c-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726165,\n",
      "    \"modificationTime\": 1686413205722,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167451,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-489.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-480.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1437.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1343.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3031,\\\"DepDelay\\\":3035,\\\"ArrTime\\\":3356,\\\"ArrDelay\\\":3475}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00009-6c30192d-45cc-4819-b3ce-3e6c4de1c218-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1725812,\n",
      "    \"modificationTime\": 1686413205670,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167448,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-990.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-706.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1430.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2996,\\\"DepDelay\\\":2999,\\\"ArrTime\\\":3289,\\\"ArrDelay\\\":3416}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00010-bff9f938-783e-490b-b462-636415a9f94d-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726428,\n",
      "    \"modificationTime\": 1686413205698,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167477,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-43.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-94.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1467.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3017,\\\"DepDelay\\\":3021,\\\"ArrTime\\\":3289,\\\"ArrDelay\\\":3388}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00011-eeea5de3-5e91-47df-9762-7d673770f769-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1631447,\n",
      "    \"modificationTime\": 1686413205710,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":157947,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-63.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-81.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1434.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1007.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2806,\\\"DepDelay\\\":2812,\\\"ArrTime\\\":3082,\\\"ArrDelay\\\":3182}}\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/storage/data/airline_2m.delta/_delta_log/00000000000000000000.json', 'r') as json_file:\n",
    "    for line in json_file:\n",
    "      json_object = json.loads(line) \n",
    "      print(json.dumps(json_object, indent=2))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|2017-07-26 00:00:00|               AA|                              9|   JFK| SFO|    657|    -3.0|    947|   -33.0|\n",
      "|2017-07-26 00:00:00|               DL|                           2051|   JFK| SJU|   2036|     7.0|     17|   -26.0|\n",
      "|2017-07-26 00:00:00|               B6|                           1089|   JFK| MCO|   1340|    21.0|   1638|    27.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf.filter('Origin=\"JFK\" and FlightDate = \"2017-07-26\"').show() # filter for flights leaving JFK on the 26th July, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|2017-07-26 00:00:00|               AA|                              9|   JFK| SFO|    756|    -4.0|   1117|     0.0|\n",
      "|2017-07-26 00:00:00|               DL|                           1368|   JFK| MIA|   1107|     1.0|   1421|     0.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new table for upsert\n",
    "sdf.dtypes\n",
    "\n",
    "updates = [\n",
    "    (datetime.strptime('2017-07-26', '%Y-%m-%d'), 'AA',9, 'JFK', 'SFO', 756, -4.0, 1117, 0.0), # update existing entry\n",
    "    (datetime.strptime('2017-07-26', '%Y-%m-%d'), 'DL',1368, 'JFK', 'MIA', 1107, 1.0, 1421, 0.0), # new entry\n",
    "]\n",
    "(updates_table := spark.createDataFrame(updates, schema=sdf.schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform upsert\n",
    "(delta_table.alias('current_data')\n",
    "  .merge(\n",
    "      source=updates_table.alias('new_data'), \n",
    "      condition=F.expr('current_data.FlightDate = new_data.FlightDate and new_data.Origin = current_data.Origin and current_data.Dest = new_data.Dest and  current_data.Reporting_Airline = new_data.Reporting_Airline and current_data.Flight_Number_Reporting_Airline = new_data.Flight_Number_Reporting_Airline'))\n",
    "  .whenMatchedUpdate(set = {\n",
    "    'DepTime': F.col('new_data.DepTime'),\n",
    "    'DepDelay': F.col('new_data.DepDelay'),\n",
    "    'ArrTime': F.col('new_data.ArrTime'),\n",
    "    'ArrDelay': F.col('new_data.ArrDelay'),\n",
    "  })\n",
    "  # .whenNotMatchedInsert(values = {\n",
    "  #   'FlightDate': 'new_data.FlightDate',\n",
    "  #   'Reporting_Airline': 'new_data.Reporting_Airline',\n",
    "  #   'Flight_Number_Reporting_Airline': 'new_data.Flight_Number_Reporting_Airline',\n",
    "  #   'DepTime': 'new_data.DepTime',\n",
    "  #   'DepDelay': 'new_data.DepDelay',\n",
    "  #   'ArrTime': 'new_data.ArrTime',\n",
    "  #   'ArrDelay': 'new_data.ArrDelay',\n",
    "  # })\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute()\n",
    "\n",
    "  # can have any number of whenMatched (at most one update and one delete action)\n",
    "  # update in merge only updates sepcified columns\n",
    "  # multiple whenMatched executes in order specified\n",
    "  # to update all columns of target dleta table use whenMatched(...).updateAll()\n",
    "\n",
    "\n",
    "  # whenNotMatched when source doesn't match target\n",
    "  # can have ONLY the insert action, any unspecified columns assume null\n",
    "  # each whenNotMatched can have optional conditoin, \n",
    "  # see https://docs.delta.io/latest/delta-update.html#language-python\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|2017-07-26 00:00:00|               DL|                           2051|   JFK| SJU|   2036|     7.0|     17|   -26.0|\n",
      "|2017-07-26 00:00:00|               B6|                           1089|   JFK| MCO|   1340|    21.0|   1638|    27.0|\n",
      "|2017-07-26 00:00:00|               DL|                           1368|   JFK| MIA|   1107|     1.0|   1421|     0.0|\n",
      "|2017-07-26 00:00:00|               AA|                              9|   JFK| SFO|    756|    -4.0|   1117|     0.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.toDF().filter('Origin=\"JFK\" and FlightDate = \"2017-07-26\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'a': ['A', 'B', 'C', 'D'],\n",
    "  'value': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|value|\n",
      "+---+-----+\n",
      "|  A|    1|\n",
      "|  B|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf.filter(F.col('a').isin(['A', 'B'])).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Starts Here\n",
    "\n",
    "\n",
    "This post covers the Delta Lake, which is an open-source format extending parquet files for ACID transactions. More specifically, this covers how to work with Delta tables using the `pyspark` and native `Delta` APIs. \n",
    "\n",
    "Delta tables can be thought of as having the benefits of a non-flat file format (compression via more efficient encoding), with a single source of truth called the transaction log."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Delta Table\n",
    "\n",
    "In order to create a delta table, I'm loading an existing CSV using `pyspark`, and saving it using the `format` option in `pyspark`'s `write`:\n",
    "\n",
    "(Completely irrelevant, however the dataset being used here is [IBM's Airline Reporting Carrier On-Timer Performance Dataset](https://developer.ibm.com/exchanges/data/all/airline/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 91.85% for 8 writers\n",
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 81.64% for 9 writers\n",
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 73.48% for 10 writers\n",
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 66.80% for 11 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 61.23% for 12 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 66.80% for 11 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 73.48% for 10 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 81.64% for 9 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 91.85% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load original dataset\n",
    "sdf = spark.read.load(\n",
    "  '/storage/data/airline_2m.csv' ,\n",
    "  format='com.databricks.spark.csv',\n",
    "  header='true',\n",
    "  inferSchema='true'\n",
    ").select(['FlightDate', 'Reporting_Airline', 'Flight_Number_Reporting_Airline','Origin', 'Dest', 'DepTime', 'DepDelay', 'ArrTime', 'ArrDelay' ]).filter('Origin=\"JFK\" and FlightDate>=\"2017-12-01\" and FlightDate <= \"2017-12-31\"')\n",
    "\n",
    "# write as a delta table\n",
    "sdf.write.format('delta').mode('overwrite').save('/storage/data/airline_2m.delta')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `CTAS` statement in sql:\n",
    "(note that this is for the purposes of demonstration and new_table is not used for the rest of this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 09:18:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 09:18:57 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 88.60% for 8 writers\n",
      "23/06/24 09:18:57 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 78.75% for 9 writers\n",
      "23/06/24 09:18:57 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 70.88% for 10 writers\n",
      "23/06/24 09:18:57 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 64.44% for 11 writers\n",
      "23/06/24 09:18:57 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 59.07% for 12 writers\n",
      "23/06/24 09:19:10 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 64.44% for 11 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====>                                                    (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 09:19:10 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 70.88% for 10 writers\n",
      "23/06/24 09:19:10 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 78.75% for 9 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==============>                                           (3 + 9) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 09:19:10 WARN MemoryManager: Total allocation exceeds 95.00% (951,320,564 bytes) of heap memory\n",
      "Scaling row group sizes to 88.60% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "  CREATE OR REPLACE TABLE new_table\n",
    "  using DELTA AS SELECT * FROM csv.`/storage/data/airline_2m.csv`\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we inspect the path which was written to, there are two things to note:\n",
    "1. There are multiple `.parquet` files\n",
    "2. There's a directory called the `_delta_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m_delta_log\u001b[0m/\n",
      "part-00000-9db93c29-a618-4f69-aa5f-776e1ca1a221-c000.snappy.parquet\n",
      "part-00001-43218537-207b-4569-8d98-7cb1d2959d3d-c000.snappy.parquet\n",
      "part-00002-b41a2670-c5bc-4515-93c6-c9fe87c3d132-c000.snappy.parquet\n",
      "part-00003-0393fe9a-e8cc-4c69-83a4-e11828b75886-c000.snappy.parquet\n",
      "part-00004-edbda9cf-91b8-4752-bec3-f30e93651fe8-c000.snappy.parquet\n",
      "part-00005-9c275ad8-871a-4948-9630-40aef37c3d50-c000.snappy.parquet\n",
      "part-00006-d273f657-9c1f-4dd1-8bbf-fb66eba644f3-c000.snappy.parquet\n",
      "part-00007-91fbd325-4e1c-437f-a7c4-e0fd8e91b26d-c000.snappy.parquet\n",
      "part-00008-d6982b42-0653-4ef5-8b00-72f3bb62b7ce-c000.snappy.parquet\n",
      "part-00009-4539d215-00c7-4f2b-9fc7-cf8c7544070c-c000.snappy.parquet\n",
      "part-00010-4238061e-7d3c-43d5-9d29-9b4291b38d55-c000.snappy.parquet\n",
      "part-00011-00828917-003b-4eff-a175-81b3e86890cb-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%ls /storage/data/airline_2m.delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder called the `_delta_log` is the single source of truth for the delta table, and contains all history for a given table; currently there is a single `.json` file, since only one operation was done to this table. This folder is automatically created when a table is created, and is updated for every operation on the delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "%ls /storage/data/airline_2m.delta/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+\n",
      "|                 add|          commitInfo|            metaData|protocol|\n",
      "+--------------------+--------------------+--------------------+--------+\n",
      "|                null|{Apache-Spark/3.3...|                null|    null|\n",
      "|                null|                null|                null|  {1, 2}|\n",
      "|                null|                null|{1687486190991, {...|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "+--------------------+--------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(jdf := spark.read.json(\"/storage/data/airline_2m.delta/_delta_log/00000000000000000000.json\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a bit hard to see, to let's filter for the one entry where `commitInfo` is not null. In the commit info, there are several important parameters, namely the ovewrite mode, the operation (in this case `WRITE`) and the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|commitInfo                                                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{Apache-Spark/3.3.2 Delta-Lake/2.3.0, false, Serializable, WRITE, {12, 33238, 75}, {Overwrite, []}, 1687486192334, 0e2eefc4-557d-45b2-ac9f-e1f56b484fbc}|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "jdf.filter(F.col('commitInfo').isNotNull()).select('commitInfo').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata stores information on the columns, type of columns, constraints on the columns and the type of file (parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metaData                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{1687486190991, {parquet}, e0aa9322-351f-40a6-9327-5d72059b0a0c, [], {\"type\":\"struct\",\"fields\":[{\"name\":\"FlightDate\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Reporting_Airline\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Flight_Number_Reporting_Airline\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Origin\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Dest\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DepTime\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DepDelay\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ArrTime\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ArrDelay\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}}|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.filter(F.col('metaData').isNotNull()).select('metaData').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more concise way of viewing this is by calling `.history().show()` on the delta table. Here we see metadata:\n",
    "- Version of the table\n",
    "- Timestamp of the given operation\n",
    "- UserID, Name (person/entity that performed the operation)\n",
    "- Operation Parameters: write mode, other config operations\n",
    "- Notebook/ClusterID: these are databricks specific and would not be populated here\n",
    "- Isolation Level: this controls write conflicts, and the degree to which a transaction must be isolated from modifications made by concurrent operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2023-06-22 22:09:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 12, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load table\n",
    "delta_table = DeltaTable.forPath(spark, '/storage/data/airline_2m.delta/')\n",
    "delta_table.history().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating a Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to modify the table in any way, such as by adding a new row, this would be recorded in the transaction-log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.toDF().count() # before the modification, we have 75 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new column which is a function of `DepDelay` and `ArrDelay`. For example, say we want to observe the relationship between Departure and Arrival delays by menas of a ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|dep_to_arr_ratio|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "|2017-12-17 00:00:00|               DL|                           2634|   JFK| MCO|   1121|    -4.0|   1359|   -32.0|           0.125|\n",
      "|2017-12-07 00:00:00|               B6|                            161|   JFK| SMF|   1820|   -10.0|   2201|    -3.0|           3.333|\n",
      "|2017-12-11 00:00:00|               DL|                           2498|   JFK| CHS|    749|   -16.0|    948|   -51.0|           0.314|\n",
      "|2017-12-21 00:00:00|               DL|                            456|   JFK| SLC|   1040|    -7.0|   1404|   -16.0|           0.438|\n",
      "|2017-12-01 00:00:00|               B6|                            415|   JFK| SFO|    921|    -8.0|   1307|    -3.0|           2.667|\n",
      "|2017-12-16 00:00:00|               DL|                            496|   JFK| SFO|   1928|     3.0|   2246|   -36.0|          -0.083|\n",
      "|2017-12-01 00:00:00|               B6|                           1507|   JFK| IAD|   2138|     3.0|   2310|    17.0|           0.176|\n",
      "|2017-12-05 00:00:00|               B6|                            286|   JFK| ROC|    741|    -9.0|    906|    -3.0|             3.0|\n",
      "|2017-12-27 00:00:00|               B6|                           1407|   JFK| IAD|    953|     0.0|   1107|    -7.0|             0.0|\n",
      "|2017-12-28 00:00:00|               B6|                            711|   JFK| LAS|   1759|     0.0|   2046|   -13.0|             0.0|\n",
      "|2017-12-05 00:00:00|               B6|                           1273|   JFK| CHS|    733|    -7.0|    943|    -8.0|           0.875|\n",
      "|2017-12-03 00:00:00|               DL|                            424|   JFK| LAX|    820|     5.0|   1131|    -4.0|           -1.25|\n",
      "|2017-12-01 00:00:00|               DL|                            451|   JFK| ATL|    557|    -3.0|    807|   -34.0|           0.088|\n",
      "|2017-12-25 00:00:00|               B6|                            583|   JFK| MCO|    658|    -2.0|   1031|    31.0|          -0.065|\n",
      "|2017-12-31 00:00:00|               VX|                           1411|   JFK| LAX|   1417|    77.0|   1806|    98.0|           0.786|\n",
      "|2017-12-07 00:00:00|               B6|                           1273|   JFK| CHS|    736|    -4.0|    956|     5.0|            -0.8|\n",
      "|2017-12-27 00:00:00|               B6|                           1013|   JFK| LGB|   1833|    33.0|   2155|    37.0|           0.892|\n",
      "|2017-12-15 00:00:00|               DL|                           2793|   JFK| LAS|    842|    12.0|   1112|   -30.0|            -0.4|\n",
      "|2017-12-01 00:00:00|               DL|                           2791|   JFK| CHS|   1353|     0.0|   1557|   -23.0|             0.0|\n",
      "|2017-12-29 00:00:00|               B6|                            108|   JFK| PWM|   2333|    48.0|     37|    39.0|           1.231|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "delta_table = spark.read.format('delta').load('/storage/data/airline_2m.delta/')\n",
    "\n",
    "delta_table_updated = (\n",
    "    delta_table\n",
    "      .withColumn('dep_to_arr_ratio', F.expr('round(DepDelay/ArrDelay, 3)'))\n",
    ")\n",
    "\n",
    "delta_table_updated.show() # great, let's write this back to the delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: e0aa9322-351f-40a6-9327-5d72059b0a0c).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- FlightDate: timestamp (nullable = true)\n-- Reporting_Airline: string (nullable = true)\n-- Flight_Number_Reporting_Airline: integer (nullable = true)\n-- Origin: string (nullable = true)\n-- Dest: string (nullable = true)\n-- DepTime: integer (nullable = true)\n-- DepDelay: double (nullable = true)\n-- ArrTime: integer (nullable = true)\n-- ArrDelay: double (nullable = true)\n\n\nData schema:\nroot\n-- FlightDate: timestamp (nullable = true)\n-- Reporting_Airline: string (nullable = true)\n-- Flight_Number_Reporting_Airline: integer (nullable = true)\n-- Origin: string (nullable = true)\n-- Dest: string (nullable = true)\n-- DepTime: integer (nullable = true)\n-- DepDelay: double (nullable = true)\n-- ArrTime: integer (nullable = true)\n-- ArrDelay: double (nullable = true)\n-- dep_to_arr_ratio: double (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m delta_table_updated\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mdelta\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m/storage/data/airline_2m.delta/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: e0aa9322-351f-40a6-9327-5d72059b0a0c).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- FlightDate: timestamp (nullable = true)\n-- Reporting_Airline: string (nullable = true)\n-- Flight_Number_Reporting_Airline: integer (nullable = true)\n-- Origin: string (nullable = true)\n-- Dest: string (nullable = true)\n-- DepTime: integer (nullable = true)\n-- DepDelay: double (nullable = true)\n-- ArrTime: integer (nullable = true)\n-- ArrDelay: double (nullable = true)\n\n\nData schema:\nroot\n-- FlightDate: timestamp (nullable = true)\n-- Reporting_Airline: string (nullable = true)\n-- Flight_Number_Reporting_Airline: integer (nullable = true)\n-- Origin: string (nullable = true)\n-- Dest: string (nullable = true)\n-- DepTime: integer (nullable = true)\n-- DepDelay: double (nullable = true)\n-- ArrTime: integer (nullable = true)\n-- ArrDelay: double (nullable = true)\n-- dep_to_arr_ratio: double (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
     ]
    }
   ],
   "source": [
    "delta_table_updated.write.mode('overwrite').format('delta').save('/storage/data/airline_2m.delta/') # Hmm, that's not right!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we get a schema mismatch error, this is because the original delta table did not contain our newly engineered column and implicity enforces schema validation. In order to help identify the error, `spark` prints out the before and after schemas of the delta table. This is an incresibly stringent check, and can be used as a gatekeeper of clean, full transformed data that is ready for production. Schema enforcement prevents data \"dilution\" which can occur when new columns are added so frequently that the original data loses its meaning due to data deluge. \n",
    "\n",
    "\n",
    "If you decide that you absolutely needed to add a new column, we use `mode('overWriteSchema', 'true')` in the write statement, also known as *schema evolution* in the Delta table documentation. There are two types of operations supported via schema *evolution*: adding new columns (what we're doing) and changing data types from `Null` to any other type OR upcasting `Byte` to `Short` or `Integer`.\n",
    "\n",
    "*Note, using the option `overwrite` here marks the original version of the data as \"tombstone\", which will cause this version to be removed if Delta's `VACUUM` command is run.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_updated.write.mode('overwrite').option('overwriteSchema', 'true').format('delta').save('/storage/data/airline_2m.delta/') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we re-examine the delta-log, there are two files present here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json  00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "%ls /storage/data/airline_2m.delta/_delta_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we view the delta-table history, we can see that a new `version` has been added, with a new timestamp and the type of operation. (Here we only select a few relevant columns for ease-of-viewing). In the `operationParameters` field, we can see where we passed in `mode`, and the in `operationMetrics` field, we can see that the number of parquet files has not changed (we didn't expect this to change) and neither did the number of rows. However, the `numOutputBytes` has changed due to the addition of our new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+--------------------------------------+--------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                   |operationMetrics                                              |\n",
      "+-------+-----------------------+---------+--------------------------------------+--------------------------------------------------------------+\n",
      "|1      |2023-06-24 09:26:21.574|WRITE    |{mode -> Overwrite, partitionBy -> []}|{numFiles -> 12, numOutputRows -> 75, numOutputBytes -> 37214}|\n",
      "|0      |2023-06-22 22:09:52.338|WRITE    |{mode -> Overwrite, partitionBy -> []}|{numFiles -> 12, numOutputRows -> 75, numOutputBytes -> 33238}|\n",
      "+-------+-----------------------+---------+--------------------------------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe history delta.`/storage/data/airline_2m.delta/`').select(*['version', 'timestamp', 'operation', 'operationParameters', 'operationMetrics']).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert\n",
    "\n",
    "This is hyper-specific to delta table syntax, where a combination of an `insert` and an `update` is done in one go. Let's say that I have a new table with updates to apply to the delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|dep_to_arr_ratio|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "|2017-12-17 00:00:00|               DL|                           2634|   JFK| MCO|   1125|     0.0|   1359|   -32.0|             0.0|\n",
      "|2017-07-26 00:00:00|               DL|                           1368|   JFK| MIA|   1107|     1.0|   1421|     1.0|             1.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# create an updates dataframe\n",
    "updates = [\n",
    "    (datetime.strptime('2017-12-17', '%Y-%m-%d'), 'DL', 2634, 'JFK', 'MCO', 1125, 0.0, 1359, -32.0, 0.0), # update existing entry to have no departure delay\n",
    "    (datetime.strptime('2017-12-26', '%Y-%m-%d'), 'DL',1368, 'JFK', 'MIA', 1107, 1.0, 1421, 1.0, 1.0), # new entry that did not exists\n",
    "]\n",
    "(updates_table := spark.createDataFrame(updates, schema=delta_table_updated.schema)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new data has both a row that matches an existing entry in our dataframe, as well as a new row that did not previously exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "|FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|dep_to_arr_ratio|\n",
      "+----------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "+----------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table_updated.filter('Flight_Number_Reporting_Airline=1368').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the syntax for upserting:\n",
    "```python\n",
    "table.alias('old_table')\n",
    "  .merge(\n",
    "    source=a_new_table.alias('a_new_table'),\n",
    "    condition= # keys to match on, in this case we match on FlightData, Origin, Dest, Airline and Flight-Number\n",
    "  )\n",
    "```\n",
    "Following this, there are multiple possible clauses, at least one of which is necessary:\n",
    "- `whenNotMatchedInsert`: adds new rows if no matches are made\n",
    "- `whenMatchedUpdate`: does not insert new rows, this is closest to an `UPDATE` statement in SQL\n",
    "\n",
    "So, in order to update matches:\n",
    "```python\n",
    "  .whenMatchedUpdate(set = {\n",
    "    'Column1': F.col('a_new_table.Column1'), # use value from updates for column1\n",
    "    'Column2': F.col('old_table.Column2'), # retain value from original dataset for column 2 \n",
    "  })\n",
    "```\n",
    "\n",
    "Finally, to insert data where no matches are made:\n",
    "```python\n",
    "  .whenNotMatchedInsertAll()\n",
    "```\n",
    "\n",
    "There can be any number of `whenMatched` clauses (at most one update and one delete) action. The update in the merge only updates the specified columns, and multiple `whenMatched` statements execute in the order specified. In order to update all the columns, use `whenMatched(...).updateAll()`, where `...` would be your specified key matches. In the `whenNotMatched` case, this can only have the insert operation, where any unspecified column assume null values.\n",
    "\n",
    "For the full API reference, see [here](https://docs.delta.io/latest/delta-update.html#language-python)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the actual upsert, we specify conditions to match on FlightDate, Origin, Destination, Flight-Number and Airline, updating the columns for departure/arrival time/delay. Finally, there's a statement to insert new rows where they did not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(DeltaTable.forPath(spark, '/storage/data/airline_2m.delta/').alias('current_data')\n",
    "  # specify merge conditions\n",
    "  .merge(\n",
    "      source=updates_table.alias('new_data'), \n",
    "      condition=F.expr('current_data.FlightDate = new_data.FlightDate and new_data.Origin = current_data.Origin and current_data.Dest = new_data.Dest and  current_data.Reporting_Airline = new_data.Reporting_Airline and current_data.Flight_Number_Reporting_Airline = new_data.Flight_Number_Reporting_Airline'))\n",
    "  # update data where matched\n",
    "  .whenMatchedUpdate(set = {\n",
    "    'DepTime': F.col('new_data.DepTime'),\n",
    "    'DepDelay': F.col('new_data.DepDelay'),\n",
    "    'ArrTime': F.col('new_data.ArrTime'),\n",
    "    'ArrDelay': F.col('new_data.ArrDelay'),\n",
    "  })\n",
    "  # insert where not matched\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute()\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json  00000000000000000001.json  00000000000000000002.json\n"
     ]
    }
   ],
   "source": [
    "# we can see that a third entry has been added to the transaction log\n",
    "%ls /storage/data/airline_2m.delta/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2023-06-24 09:59:...|  null|    null|    MERGE|{predicate -> (((...|null|    null|     null|          1|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-06-24 09:26:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|  Serializable|        false|{numFiles -> 12, ...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-06-22 22:09:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 12, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = DeltaTable.forPath(spark, '/storage/data/airline_2m.delta/').history()\n",
    "history.show() # that's a bit much, let's inspect individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+\n",
      "|           timestamp|version|operation|\n",
      "+--------------------+-------+---------+\n",
      "|2023-06-24 09:59:...|      2|    MERGE|\n",
      "|2023-06-24 09:26:...|      1|    WRITE|\n",
      "|2023-06-22 22:09:...|      0|    WRITE|\n",
      "+--------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.select(*['timestamp', 'version','operation']).show() # there is now a third version (number 2), with an operation MERGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`operationParameters` shows us the substance of the operation, where the `predicate` outlines our merge conditions and the defined action (in this case an `update`), followed by a predicate for non-matched rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|operationParameters                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{predicate -> ((((current_data.FlightDate = new_data.FlightDate) AND (new_data.Origin = current_data.Origin)) AND (current_data.Dest = new_data.Dest)) AND ((current_data.Reporting_Airline = new_data.Reporting_Airline) AND (current_data.Flight_Number_Reporting_Airline = new_data.Flight_Number_Reporting_Airline))), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.filter('version=2').select('operationParameters').show(truncate=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we view operation metrics, we get a readout for the number of rows added/deleted, files added/deleted, the number of input/output rows etc etc. This is the most useful for logging operations performed on a delta table, and can be used to easily catch any spurios operations, such as the addition of extra rows, schema implosion and file cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{numTargetRowsCopied -> 7, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3209, numTargetBytesRemoved -> 3218, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 875, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 515, numTargetRowsUpdated -> 1, numOutputRows -> 9, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 340}|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.filter('version=2').select('operationMetrics').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Travel\n",
    "\n",
    "Now, possibly the most useful feature of having a transaction-log is \"time-travel\". This is essentially being able to access versions of the delta at different points in its lifetime. Let's re-examine the history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|version|           timestamp|\n",
      "+-------+--------------------+\n",
      "|      2|2023-06-24 09:59:...|\n",
      "|      1|2023-06-24 09:26:...|\n",
      "|      0|2023-06-22 22:09:...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.select(*['version', 'timestamp']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see 3 versions, the original, and two operations. In order to access these previous versions, we need to use one of the (many)  ways of accessing previous revisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FlightDate: timestamp (nullable = true)\n",
      " |-- Reporting_Airline: string (nullable = true)\n",
      " |-- Flight_Number_Reporting_Airline: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading version 0, we can see that the engineered column added in version 1 does not exist as yet\n",
    "(spark.read.format('delta').option('versionAsOf', 0).load('/storage/data/airline_2m.delta/')).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read version 1 using slightly different syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|version|timestamp              |\n",
      "+-------+-----------------------+\n",
      "|2      |2023-06-24 09:59:29.027|\n",
      "|1      |2023-06-24 09:26:21.574|\n",
      "|0      |2023-06-22 22:09:52.338|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.select(*['version', 'timestamp']).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, our new column has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FlightDate: timestamp (nullable = true)\n",
      " |-- Reporting_Airline: string (nullable = true)\n",
      " |-- Flight_Number_Reporting_Airline: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- dep_to_arr_ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read.format('delta').option('timestampAsOf', '2023-06-24 09:26:21.574').load('/storage/data/airline_2m.delta/')).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we view version 2, we can see the difference in the number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 76)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(spark.read.format('delta').option('versionAsOf', 1).load('/storage/data/airline_2m.delta/')).count(), (spark.read.format('delta').option('versionAsOf', 2).load('/storage/data/airline_2m.delta/')).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vacuum\n",
    "Time-travel is enabled via the transaction log, however it would be inefficient to store every version of every table forever. As aforementioned, using the `overwrite` option marks previous versions of data for being removed via the `vacuum` command. The other condition for removing files via `vacuum` is files existing after some given time period (by default it's 7 days). If we observe the delta directory now (after three operations):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "  /home ❯ tree /storage/data/airline_2m.delta                                  at  10:43:52\n",
    "/storage/data/airline_2m.delta\n",
    "├── _delta_log\n",
    "│   ├── 00000000000000000000.json\n",
    "│   ├── 00000000000000000001.json\n",
    "│   └── 00000000000000000002.json\n",
    "├── part-00000-716191cb-67cf-4e63-a06d-c3943fb45664-c000.snappy.parquet\n",
    "├── part-00000-74403ae3-1703-4c27-aa4c-7de9bd1e6fbe-c000.snappy.parquet\n",
    "├── part-00000-9db93c29-a618-4f69-aa5f-776e1ca1a221-c000.snappy.parquet\n",
    "├── part-00001-43218537-207b-4569-8d98-7cb1d2959d3d-c000.snappy.parquet\n",
    "├── part-00001-e1a760f6-c163-4aa5-b327-fd97f40d8509-c000.snappy.parquet\n",
    "├── part-00002-17bd3789-0052-4a37-aa0a-bd91b4b1fbc7-c000.snappy.parquet\n",
    "├── part-00002-b41a2670-c5bc-4515-93c6-c9fe87c3d132-c000.snappy.parquet\n",
    "├── part-00003-0393fe9a-e8cc-4c69-83a4-e11828b75886-c000.snappy.parquet\n",
    "├── part-00003-bad613ae-6925-4578-9c0e-dfd43d31a8f7-c000.snappy.parquet\n",
    "├── part-00004-4846b914-1702-446b-861a-52c9e9f080f0-c000.snappy.parquet\n",
    "├── part-00004-edbda9cf-91b8-4752-bec3-f30e93651fe8-c000.snappy.parquet\n",
    "├── part-00005-61394241-d08a-4dbc-9f9e-8a2a904c0a18-c000.snappy.parquet\n",
    "├── part-00005-9c275ad8-871a-4948-9630-40aef37c3d50-c000.snappy.parquet\n",
    "├── part-00006-1829cf9d-4451-4882-9587-2a92cfdff619-c000.snappy.parquet\n",
    "├── part-00006-d273f657-9c1f-4dd1-8bbf-fb66eba644f3-c000.snappy.parquet\n",
    "├── part-00007-91fbd325-4e1c-437f-a7c4-e0fd8e91b26d-c000.snappy.parquet\n",
    "├── part-00007-aaf31339-c029-4955-80f5-2649b4fe6caf-c000.snappy.parquet\n",
    "├── part-00008-2340179d-0109-4c50-89ba-f31242beba14-c000.snappy.parquet\n",
    "├── part-00008-d6982b42-0653-4ef5-8b00-72f3bb62b7ce-c000.snappy.parquet\n",
    "├── part-00009-4539d215-00c7-4f2b-9fc7-cf8c7544070c-c000.snappy.parquet\n",
    "├── part-00009-5b136823-d160-403d-b8aa-6fe6126aad2d-c000.snappy.parquet\n",
    "├── part-00010-4238061e-7d3c-43d5-9d29-9b4291b38d55-c000.snappy.parquet\n",
    "├── part-00010-c8695154-ff81-43d7-b7c3-f2687192ce7a-c000.snappy.parquet\n",
    "├── part-00011-00828917-003b-4eff-a175-81b3e86890cb-c000.snappy.parquet\n",
    "└── part-00011-7fc1ae81-7e4e-4dfd-b8be-fd5a0c43e127-c000.snappy.parquet\n",
    "\n",
    "1 directory, 28 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n",
      "+-------------------------------------------------------------+\n",
      "|path                                                         |\n",
      "+-------------------------------------------------------------+\n",
      "|file:/storage/projects/notes/tools/spark-warehouse/airline_2m|\n",
      "+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we need to disable some safety checks\n",
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled','false')\n",
    "spark.sql('vacuum airline_2m retain 0 hours').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above would show files removed if there were older files in the delta-history. The safety check is disabled since running `VACUUM` for files which existed less than a week is generally note desirable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
