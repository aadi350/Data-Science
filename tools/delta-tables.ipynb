{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Hands-On Guide to Delta Tables (in Python)\n",
    "\n",
    "Delta is a versioned parquet file with a transaction log, we get compression and all benefits of parquet  \n",
    "Transaction log is single-source of truth, see who does what, plays nicely with spark and python  \n",
    "\n",
    "\n",
    "Python APIs for Delta-Lake\n",
    "- `pyspark`\n",
    "- `delta-rs` pip install delta-lake\n",
    "- pyspark declarative `pip install delta-spark`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder.appName('delta-tutorial').config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 94.30% for 8 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 83.83% for 9 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 75.44% for 10 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 68.58% for 11 writers\n",
      "23/06/10 12:36:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 62.87% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/10 12:36:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 68.58% for 11 writers\n",
      "23/06/10 12:36:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 75.44% for 10 writers\n",
      "23/06/10 12:36:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 83.83% for 9 writers\n",
      "23/06/10 12:36:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,012,583,616 bytes) of heap memory\n",
      "Scaling row group sizes to 94.30% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load vanilla spark table\n",
    "sdf = spark.read.load(\n",
    "  '/storage/data/airline_2m.csv' ,\n",
    "  format='com.databricks.spark.csv',\n",
    "  header='true',\n",
    "  inferSchema='true'\n",
    ").select(['FlightDate', 'Reporting_Airline', 'Flight_Number_Reporting_Airline','Origin', 'Dest', 'DepTime', 'DepDelay', 'ArrTime', 'ArrDelay' ])\n",
    "\n",
    "# save as a delta table\n",
    "sdf.write.format('delta').mode('overwrite').save('/storage/data/airline_2m.delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, '/storage/data/airline_2m.delta/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00008-37835621-905f-481b-bee1-f2fbde02595c-c000.snappy.parquet',\n",
       " '.part-00002-f79474ae-503d-4914-8aff-54af17da6de2-c000.snappy.parquet.crc',\n",
       " '.part-00006-9cdf358f-5ff7-4c45-9314-09aeeb770ab4-c000.snappy.parquet.crc',\n",
       " '.part-00001-da20e657-cbd5-4ca0-b7bf-f862a6083d2b-c000.snappy.parquet.crc',\n",
       " 'part-00007-3e4232fc-fbd4-4cce-84fb-7a862aff9928-c000.snappy.parquet',\n",
       " '.part-00010-bff9f938-783e-490b-b462-636415a9f94d-c000.snappy.parquet.crc',\n",
       " '.part-00009-6c30192d-45cc-4819-b3ce-3e6c4de1c218-c000.snappy.parquet.crc',\n",
       " 'part-00004-016a4d0d-6026-4cff-9c53-8a4af7c32241-c000.snappy.parquet',\n",
       " 'part-00005-3885ce51-666c-4fb2-a0ad-a12c90ec095a-c000.snappy.parquet',\n",
       " '_delta_log',\n",
       " 'part-00003-9aec4e25-09dd-4f1d-8bff-e6c7c368ae7d-c000.snappy.parquet',\n",
       " '.part-00008-37835621-905f-481b-bee1-f2fbde02595c-c000.snappy.parquet.crc',\n",
       " 'part-00006-9cdf358f-5ff7-4c45-9314-09aeeb770ab4-c000.snappy.parquet',\n",
       " 'part-00009-6c30192d-45cc-4819-b3ce-3e6c4de1c218-c000.snappy.parquet',\n",
       " '.part-00003-9aec4e25-09dd-4f1d-8bff-e6c7c368ae7d-c000.snappy.parquet.crc',\n",
       " 'part-00002-f79474ae-503d-4914-8aff-54af17da6de2-c000.snappy.parquet',\n",
       " 'part-00010-bff9f938-783e-490b-b462-636415a9f94d-c000.snappy.parquet',\n",
       " 'part-00011-eeea5de3-5e91-47df-9762-7d673770f769-c000.snappy.parquet',\n",
       " '.part-00011-eeea5de3-5e91-47df-9762-7d673770f769-c000.snappy.parquet.crc',\n",
       " '.part-00007-3e4232fc-fbd4-4cce-84fb-7a862aff9928-c000.snappy.parquet.crc',\n",
       " '.part-00005-3885ce51-666c-4fb2-a0ad-a12c90ec095a-c000.snappy.parquet.crc',\n",
       " 'part-00001-da20e657-cbd5-4ca0-b7bf-f862a6083d2b-c000.snappy.parquet',\n",
       " '.part-00000-9642bf76-d935-42ae-bbe4-64d7b1c52df2-c000.snappy.parquet.crc',\n",
       " 'part-00000-9642bf76-d935-42ae-bbe4-64d7b1c52df2-c000.snappy.parquet',\n",
       " '.part-00004-016a4d0d-6026-4cff-9c53-8a4af7c32241-c000.snappy.parquet.crc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting what we just wrote\n",
    "import os\n",
    "\n",
    "os.listdir('/storage/data/airline_2m.delta/')\n",
    "# bunch of individual files and a folder called \"_delta_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00000000000000000000.json', '.00000000000000000000.json.crc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _delta_log\n",
    "os.listdir('/storage/data/airline_2m.delta/_delta_log/')\n",
    "# a json and a .crc file, crc files are checksums added to prevent corruption if parquet is corrupted in-flight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logs hold:\n",
    "copy-paste:\n",
    "Whenever a user performs an operation to modify a table (such as an INSERT, UPDATE or DELETE), Delta Lake breaks that operation down into a series of discrete steps composed of one or more of the actions below.\n",
    "\n",
    "Add file - adds a data file.\n",
    "Remove file - removes a data file.\n",
    "Update metadata - Updates the table’s metadata (e.g., changing the table’s name, schema or partitioning).\n",
    "Set transaction - Records that a structured streaming job has committed a micro-batch with the given ID.\n",
    "Change protocol - enables new features by switching the Delta Lake transaction log to the newest software protocol.\n",
    "Commit info - Contains information around the commit, which operation was made, from where and at what time.\n",
    "\n",
    "\n",
    "WHen table is created, table's transaciton log automatically created in `_delta_log`. Each change is recorded as an atomic commit in the transaction log\n",
    "\n",
    "Time-travel `DESCRIBE HISTORY` in SQL, can select fomr using `TIMESTAMP` OR the `VERSION` `TABLE@v2` \n",
    "\n",
    "\n",
    "Delta transaciton enables ACID, full audit and scalable metadata.\n",
    "\n",
    "Hive metastore stores table definition, where table is stored\n",
    "\n",
    "\n",
    "# CTAS Statements\n",
    "`CREATE_TABLE _ AS SELECT` use output of select to crae\n",
    "\n",
    "Does not support manual schema declaration, automatically infers schema from query results, does not require an `INSERT` statement.\n",
    "\n",
    "```CRETE TABLE new_table\n",
    "COMMENT \"some comment\"\n",
    "PARTITIONED BY (id1, id2) --best practice to non-partition\n",
    "LOCATION '/some/path'\n",
    "FROM ...\n",
    "```\n",
    "\n",
    "# Constraints\n",
    "- NOT NULL and CHECK supported\n",
    "`CHECK` looks like `WHERE` clauses, `NOT NULL` is obvious\n",
    "\n",
    "# Copying\n",
    "**DEEP** clone  \n",
    "`CREATE TABLE table_clone DEEP CLONE source_table` can occur incrementall (done multiple times)\n",
    "\n",
    "**SHALLOW** clone  \n",
    "only copies transaction log, no actual data copied  \n",
    "\n",
    "\n",
    "# Things to observe \n",
    "- Updates\n",
    "- Delete\n",
    "- Optimize: compacts multiple small files\n",
    "- ZORDER BY: added to optimize, this compacts files ordered by the column, like indexing\n",
    "    - Zordering for high-cardinality columns (>= 2 columns): can I go ahead and skip (i.e. reduce number of files need to scan), diminishing returns if z-order by all columns\n",
    "    - Partitioning is for low-cardinality columns, table >= 1TB of data, partitions >= 1GB\n",
    "- Cleaning up: delta lake `VACUUM table_name [retention period]`, feault retention period is 7 days. Once vacuum is run, time-travel is lost\n",
    "- Try deleting then using time-travel to go back using `RESTORE TABLE` then calling `DESCRIBE HISTORY`  \n",
    "\n",
    "can call `delta_table.history().show()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\": {\n",
      "    \"timestamp\": 1686413205823,\n",
      "    \"operation\": \"WRITE\",\n",
      "    \"operationParameters\": {\n",
      "      \"mode\": \"Overwrite\",\n",
      "      \"partitionBy\": \"[]\"\n",
      "    },\n",
      "    \"isolationLevel\": \"Serializable\",\n",
      "    \"isBlindAppend\": false,\n",
      "    \"operationMetrics\": {\n",
      "      \"numFiles\": \"12\",\n",
      "      \"numOutputRows\": \"2000000\",\n",
      "      \"numOutputBytes\": \"20619620\"\n",
      "    },\n",
      "    \"engineInfo\": \"Apache-Spark/3.3.2 Delta-Lake/2.3.0\",\n",
      "    \"txnId\": \"4f5656f9-d9d2-404e-bf11-722a1349387b\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"protocol\": {\n",
      "    \"minReaderVersion\": 1,\n",
      "    \"minWriterVersion\": 2\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"metaData\": {\n",
      "    \"id\": \"609a6c9c-5c5d-4844-9276-49201985b8f1\",\n",
      "    \"format\": {\n",
      "      \"provider\": \"parquet\",\n",
      "      \"options\": {}\n",
      "    },\n",
      "    \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"FlightDate\\\",\\\"type\\\":\\\"timestamp\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"Reporting_Airline\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"Origin\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"Dest\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"DepTime\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"DepDelay\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"ArrTime\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"ArrDelay\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "    \"partitionColumns\": [],\n",
      "    \"configuration\": {},\n",
      "    \"createdTime\": 1686413204081\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00000-9642bf76-d935-42ae-bbe4-64d7b1c52df2-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726172,\n",
      "    \"modificationTime\": 1686413205702,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167450,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-60.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-89.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1153.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3012,\\\"DepDelay\\\":3016,\\\"ArrTime\\\":3309,\\\"ArrDelay\\\":3446}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00001-da20e657-cbd5-4ca0-b7bf-f862a6083d2b-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726016,\n",
      "    \"modificationTime\": 1686413205734,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167446,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-82.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-80.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1434.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1295.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2997,\\\"DepDelay\\\":3005,\\\"ArrTime\\\":3307,\\\"ArrDelay\\\":3436}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00002-f79474ae-503d-4914-8aff-54af17da6de2-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1725906,\n",
      "    \"modificationTime\": 1686413205706,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167449,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-62.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-90.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1855.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1847.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2998,\\\"DepDelay\\\":3009,\\\"ArrTime\\\":3304,\\\"ArrDelay\\\":3437}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00003-9aec4e25-09dd-4f1d-8bff-e6c7c368ae7d-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726098,\n",
      "    \"modificationTime\": 1686413205622,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167467,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-67.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-81.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1878.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1898.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2989,\\\"DepDelay\\\":2994,\\\"ArrTime\\\":3252,\\\"ArrDelay\\\":3388}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00004-016a4d0d-6026-4cff-9c53-8a4af7c32241-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726276,\n",
      "    \"modificationTime\": 1686413205718,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167462,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-60.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-77.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1628.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1631.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3042,\\\"DepDelay\\\":3044,\\\"ArrTime\\\":3325,\\\"ArrDelay\\\":3474}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00005-3885ce51-666c-4fb2-a0ad-a12c90ec095a-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1727251,\n",
      "    \"modificationTime\": 1686413205710,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167481,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-49.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-90.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1458.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3059,\\\"DepDelay\\\":3066,\\\"ArrTime\\\":3380,\\\"ArrDelay\\\":3519}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00006-9cdf358f-5ff7-4c45-9314-09aeeb770ab4-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726159,\n",
      "    \"modificationTime\": 1686413205670,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167477,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-62.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-76.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1430.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1402.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3069,\\\"DepDelay\\\":3073,\\\"ArrTime\\\":3357,\\\"ArrDelay\\\":3470}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00007-3e4232fc-fbd4-4cce-84fb-7a862aff9928-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1725890,\n",
      "    \"modificationTime\": 1686413205702,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167445,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-60.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-91.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1538.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1532.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2989,\\\"DepDelay\\\":2994,\\\"ArrTime\\\":3301,\\\"ArrDelay\\\":3447}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00008-37835621-905f-481b-bee1-f2fbde02595c-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726165,\n",
      "    \"modificationTime\": 1686413205722,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167451,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-489.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-480.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1437.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1343.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3031,\\\"DepDelay\\\":3035,\\\"ArrTime\\\":3356,\\\"ArrDelay\\\":3475}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00009-6c30192d-45cc-4819-b3ce-3e6c4de1c218-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1725812,\n",
      "    \"modificationTime\": 1686413205670,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167448,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-990.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-706.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1430.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2996,\\\"DepDelay\\\":2999,\\\"ArrTime\\\":3289,\\\"ArrDelay\\\":3416}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00010-bff9f938-783e-490b-b462-636415a9f94d-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1726428,\n",
      "    \"modificationTime\": 1686413205698,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":167477,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-43.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-94.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1435.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1467.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":3017,\\\"DepDelay\\\":3021,\\\"ArrTime\\\":3289,\\\"ArrDelay\\\":3388}}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"add\": {\n",
      "    \"path\": \"part-00011-eeea5de3-5e91-47df-9762-7d673770f769-c000.snappy.parquet\",\n",
      "    \"partitionValues\": {},\n",
      "    \"size\": 1631447,\n",
      "    \"modificationTime\": 1686413205710,\n",
      "    \"dataChange\": true,\n",
      "    \"stats\": \"{\\\"numRecords\\\":157947,\\\"minValues\\\":{\\\"FlightDate\\\":\\\"1987-10-01T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"9E\\\",\\\"Origin\\\":\\\"ABE\\\",\\\"Dest\\\":\\\"ABE\\\",\\\"DepTime\\\":1,\\\"DepDelay\\\":-63.0,\\\"ArrTime\\\":1,\\\"ArrDelay\\\":-81.0},\\\"maxValues\\\":{\\\"FlightDate\\\":\\\"2020-03-31T00:00:00.000-04:00\\\",\\\"Reporting_Airline\\\":\\\"YX\\\",\\\"Origin\\\":\\\"YUM\\\",\\\"Dest\\\":\\\"YUM\\\",\\\"DepTime\\\":2400,\\\"DepDelay\\\":1434.0,\\\"ArrTime\\\":2400,\\\"ArrDelay\\\":1007.0},\\\"nullCount\\\":{\\\"FlightDate\\\":0,\\\"Reporting_Airline\\\":0,\\\"Origin\\\":0,\\\"Dest\\\":0,\\\"DepTime\\\":2806,\\\"DepDelay\\\":2812,\\\"ArrTime\\\":3082,\\\"ArrDelay\\\":3182}}\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/storage/data/airline_2m.delta/_delta_log/00000000000000000000.json', 'r') as json_file:\n",
    "    for line in json_file:\n",
    "      json_object = json.loads(line) \n",
    "      print(json.dumps(json_object, indent=2))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|2017-07-26 00:00:00|               AA|                              9|   JFK| SFO|    657|    -3.0|    947|   -33.0|\n",
      "|2017-07-26 00:00:00|               DL|                           2051|   JFK| SJU|   2036|     7.0|     17|   -26.0|\n",
      "|2017-07-26 00:00:00|               B6|                           1089|   JFK| MCO|   1340|    21.0|   1638|    27.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf.filter('Origin=\"JFK\" and FlightDate = \"2017-07-26\"').show() # filter for flights leaving JFK on the 26th July, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|2017-07-26 00:00:00|               AA|                              9|   JFK| SFO|    756|    -4.0|   1117|     0.0|\n",
      "|2017-07-26 00:00:00|               DL|                           1368|   JFK| MIA|   1107|     1.0|   1421|     0.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new table for upsert\n",
    "sdf.dtypes\n",
    "\n",
    "updates = [\n",
    "    (datetime.strptime('2017-07-26', '%Y-%m-%d'), 'AA',9, 'JFK', 'SFO', 756, -4.0, 1117, 0.0), # update existing entry\n",
    "    (datetime.strptime('2017-07-26', '%Y-%m-%d'), 'DL',1368, 'JFK', 'MIA', 1107, 1.0, 1421, 0.0), # new entry\n",
    "]\n",
    "(updates_table := spark.createDataFrame(updates, schema=sdf.schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform upsert\n",
    "(delta_table.alias('current_data')\n",
    "  .merge(\n",
    "      source=updates_table.alias('new_data'), \n",
    "      condition=F.expr('current_data.FlightDate = new_data.FlightDate and new_data.Origin = current_data.Origin and current_data.Dest = new_data.Dest and  current_data.Reporting_Airline = new_data.Reporting_Airline and current_data.Flight_Number_Reporting_Airline = new_data.Flight_Number_Reporting_Airline'))\n",
    "  .whenMatchedUpdate(set = {\n",
    "    'DepTime': F.col('new_data.DepTime'),\n",
    "    'DepDelay': F.col('new_data.DepDelay'),\n",
    "    'ArrTime': F.col('new_data.ArrTime'),\n",
    "    'ArrDelay': F.col('new_data.ArrDelay'),\n",
    "  })\n",
    "  # .whenNotMatchedInsert(values = {\n",
    "  #   'FlightDate': 'new_data.FlightDate',\n",
    "  #   'Reporting_Airline': 'new_data.Reporting_Airline',\n",
    "  #   'Flight_Number_Reporting_Airline': 'new_data.Flight_Number_Reporting_Airline',\n",
    "  #   'DepTime': 'new_data.DepTime',\n",
    "  #   'DepDelay': 'new_data.DepDelay',\n",
    "  #   'ArrTime': 'new_data.ArrTime',\n",
    "  #   'ArrDelay': 'new_data.ArrDelay',\n",
    "  # })\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute()\n",
    "\n",
    "  # can have any number of whenMatched (at most one update and one delete action)\n",
    "  # update in merge only updates sepcified columns\n",
    "  # multiple whenMatched executes in order specified\n",
    "  # to update all columns of target dleta table use whenMatched(...).updateAll()\n",
    "\n",
    "\n",
    "  # whenNotMatched when source doesn't match target\n",
    "  # can have ONLY the insert action, any unspecified columns assume null\n",
    "  # each whenNotMatched can have optional conditoin, \n",
    "  # see https://docs.delta.io/latest/delta-update.html#language-python\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|         FlightDate|Reporting_Airline|Flight_Number_Reporting_Airline|Origin|Dest|DepTime|DepDelay|ArrTime|ArrDelay|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "|2017-07-26 00:00:00|               DL|                           2051|   JFK| SJU|   2036|     7.0|     17|   -26.0|\n",
      "|2017-07-26 00:00:00|               B6|                           1089|   JFK| MCO|   1340|    21.0|   1638|    27.0|\n",
      "|2017-07-26 00:00:00|               DL|                           1368|   JFK| MIA|   1107|     1.0|   1421|     0.0|\n",
      "|2017-07-26 00:00:00|               AA|                              9|   JFK| SFO|    756|    -4.0|   1117|     0.0|\n",
      "+-------------------+-----------------+-------------------------------+------+----+-------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.toDF().filter('Origin=\"JFK\" and FlightDate = \"2017-07-26\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'a': ['A', 'B', 'C', 'D'],\n",
    "  'value': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|value|\n",
      "+---+-----+\n",
      "|  A|    1|\n",
      "|  B|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf.filter(F.col('a').isin(['A', 'B'])).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Starts Here\n",
    "\n",
    "\n",
    "This post covers the Delta Lake, which is an open-source format extending parquet files for ACID transactions. More specifically, this covers how to work with Delta tables using the `pyspark` and native `Delta` APIs. \n",
    "\n",
    "Delta tables can be thought of as having the benefits of a non-flat file format (compression via more efficient encoding), with a single source of truth called the transaction log."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Delta Table\n",
    "\n",
    "In order to create a delta table, I'm loading an existing CSV using `pyspark`, and saving it using the `format` option in `pyspark`'s `write`:\n",
    "\n",
    "(Completely irrelevant, however the dataset being used here is [IBM's Airline Reporting Carrier On-Timer Performance Dataset](https://developer.ibm.com/exchanges/data/all/airline/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 91.85% for 8 writers\n",
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 81.64% for 9 writers\n",
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 73.48% for 10 writers\n",
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 66.80% for 11 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 22:09:51 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 61.23% for 12 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 66.80% for 11 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 73.48% for 10 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 81.64% for 9 writers\n",
      "23/06/22 22:09:52 WARN MemoryManager: Total allocation exceeds 95.00% (986,185,716 bytes) of heap memory\n",
      "Scaling row group sizes to 91.85% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load original dataset\n",
    "sdf = spark.read.load(\n",
    "  '/storage/data/airline_2m.csv' ,\n",
    "  format='com.databricks.spark.csv',\n",
    "  header='true',\n",
    "  inferSchema='true'\n",
    ").select(['FlightDate', 'Reporting_Airline', 'Flight_Number_Reporting_Airline','Origin', 'Dest', 'DepTime', 'DepDelay', 'ArrTime', 'ArrDelay' ]).filter('Origin=\"JFK\" and FlightDate>=\"2017-12-01\" and FlightDate <= \"2017-12-31\"')\n",
    "\n",
    "# write as a delta table\n",
    "sdf.write.format('delta').mode('overwrite').save('/storage/data/airline_2m.delta')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we inspect the path which was written to, there are two things to note:\n",
    "1. There are multiple `.parquet` files\n",
    "2. There's a directory called the `_delta_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m_delta_log\u001b[0m/\n",
      "part-00000-9db93c29-a618-4f69-aa5f-776e1ca1a221-c000.snappy.parquet\n",
      "part-00001-43218537-207b-4569-8d98-7cb1d2959d3d-c000.snappy.parquet\n",
      "part-00002-b41a2670-c5bc-4515-93c6-c9fe87c3d132-c000.snappy.parquet\n",
      "part-00003-0393fe9a-e8cc-4c69-83a4-e11828b75886-c000.snappy.parquet\n",
      "part-00004-edbda9cf-91b8-4752-bec3-f30e93651fe8-c000.snappy.parquet\n",
      "part-00005-9c275ad8-871a-4948-9630-40aef37c3d50-c000.snappy.parquet\n",
      "part-00006-d273f657-9c1f-4dd1-8bbf-fb66eba644f3-c000.snappy.parquet\n",
      "part-00007-91fbd325-4e1c-437f-a7c4-e0fd8e91b26d-c000.snappy.parquet\n",
      "part-00008-d6982b42-0653-4ef5-8b00-72f3bb62b7ce-c000.snappy.parquet\n",
      "part-00009-4539d215-00c7-4f2b-9fc7-cf8c7544070c-c000.snappy.parquet\n",
      "part-00010-4238061e-7d3c-43d5-9d29-9b4291b38d55-c000.snappy.parquet\n",
      "part-00011-00828917-003b-4eff-a175-81b3e86890cb-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%ls /storage/data/airline_2m.delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder called the `_delta_log` is the single source of truth for the delta table, and contains all history for a given table; currently there is a single `.json` file, since only one operation was done to this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "%ls /storage/data/airline_2m.delta/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+\n",
      "|                 add|          commitInfo|            metaData|protocol|\n",
      "+--------------------+--------------------+--------------------+--------+\n",
      "|                null|{Apache-Spark/3.3...|                null|    null|\n",
      "|                null|                null|                null|  {1, 2}|\n",
      "|                null|                null|{1687486190991, {...|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "|{true, 1687486192...|                null|                null|    null|\n",
      "+--------------------+--------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(jdf := spark.read.json(\"/storage/data/airline_2m.delta/_delta_log/00000000000000000000.json\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a bit hard to see, to let's filter for the one entry where `commitInfo` is not null. In the commit info, there are several important parameters, namely the ovewrite mode, the operation (in this case `WRITE`) and the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|commitInfo                                                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{Apache-Spark/3.3.2 Delta-Lake/2.3.0, false, Serializable, WRITE, {12, 33238, 75}, {Overwrite, []}, 1687486192334, 0e2eefc4-557d-45b2-ac9f-e1f56b484fbc}|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "jdf.filter(F.col('commitInfo').isNotNull()).select('commitInfo').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata stores information on the columns, type of columns, constraints on the columns and the type of file (parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metaData                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{1687486190991, {parquet}, e0aa9322-351f-40a6-9327-5d72059b0a0c, [], {\"type\":\"struct\",\"fields\":[{\"name\":\"FlightDate\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Reporting_Airline\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Flight_Number_Reporting_Airline\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Origin\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Dest\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DepTime\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DepDelay\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ArrTime\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ArrDelay\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}}|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.filter(F.col('metaData').isNotNull()).select('metaData').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to modify the table in any way, such as by adding a new row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
