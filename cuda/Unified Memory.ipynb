{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the 3 sections in **bold** above. In the next lab, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-a943-72eb-c639-ed67.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-a943-72eb-c639-ed67.qdrep\"\n",
      "Exporting 4641 events: [==================================================100%]                                                    ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-a943-72eb-c639-ed67.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average      Minimum     Maximum            Name         \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  ---------------------\n",
      "    88.4       2305897722          1  2305897722.0  2305897722  2305897722  cudaDeviceSynchronize\n",
      "    10.8        281955856          3    93985285.3       34469   281836137  cudaMallocManaged    \n",
      "     0.8         21195996          3     7065332.0     6407570     8147886  cudaFree             \n",
      "     0.0            47101          1       47101.0       47101       47101  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average      Minimum     Maximum                       Name                    \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  -------------------------------------------\n",
      "   100.0       2305887702          1  2305887702.0  2305887702  2305887702  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         68402403        2304  29688.5     1886   171392  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         20947409         768  27275.3     1119   160448  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    89.6       5470626866        277  19749555.5    52617  100137153  poll                      \n",
      "     8.4        513946551        245   2097741.0    11276   20682563  sem_timedwait             \n",
      "     1.5         92226155        680    135626.7     1108   17228096  ioctl                     \n",
      "     0.4         24058123         98    245491.1     1664    8064319  mmap                      \n",
      "     0.0          1657340         82     20211.5     4761      38856  open64                    \n",
      "     0.0           216053          3     72017.7    69710      76304  fgets                     \n",
      "     0.0           178167          4     44541.8    33233      57171  pthread_create            \n",
      "     0.0           173433         25      6937.3     1643      43307  fopen                     \n",
      "     0.0            90782         11      8252.9     4167      13173  write                     \n",
      "     0.0            71857         11      6532.5     2825      20010  munmap                    \n",
      "     0.0            36596          5      7319.2     3248       9420  open                      \n",
      "     0.0            36457         18      2025.4     1062       9660  fclose                    \n",
      "     0.0            34023          3     11341.0     3329      20303  fread                     \n",
      "     0.0            31420         20      1571.0     1005       9505  fcntl                     \n",
      "     0.0            27938          6      4656.3     1120      13090  fgetc                     \n",
      "     0.0            23368         13      1797.5     1078       2941  read                      \n",
      "     0.0            21787          1     21787.0    21787      21787  sem_wait                  \n",
      "     0.0            20380          2     10190.0    10188      10192  socket                    \n",
      "     0.0            18382          3      6127.3     2357       8671  pthread_rwlock_timedwrlock\n",
      "     0.0             9496          1      9496.0     9496       9496  connect                   \n",
      "     0.0             8592          1      8592.0     8592       8592  pipe2                     \n",
      "     0.0             8531          4      2132.8     1870       2764  mprotect                  \n",
      "     0.0             3392          1      3392.0     3392       3392  bind                      \n",
      "     0.0             2362          1      2362.0     2362       2362  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report1.qdrep\"\n",
      "Report file moved to \"/dli/task/report1.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-01a7-3cae-7141-0a9f.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-01a7-3cae-7141-0a9f.qdrep\"\n",
      "Exporting 4341 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-01a7-3cae-7141-0a9f.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    63.1        575527970          1  575527970.0  575527970  575527970  cudaDeviceSynchronize\n",
      "    34.3        312536755          3  104178918.3      26109  312422940  cudaMallocManaged    \n",
      "     2.6         23559000          3    7853000.0    7218303    9105057  cudaFree             \n",
      "     0.0            47873          1      47873.0      47873      47873  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        575574928          1  575574928.0  575574928  575574928  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         68509491        2304  29735.0     1855   171327  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         20947535         768  27275.4     1151   159519  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    86.4       2208133801        114  19369594.7    58463  100135846  poll                      \n",
      "     8.7        221713802        100   2217138.0    15668   20822590  sem_timedwait             \n",
      "     3.8         95841750        680    140943.8     1035   17384164  ioctl                     \n",
      "     1.0         26625321         98    271686.9     1560    9024686  mmap                      \n",
      "     0.1          1805716         82     22020.9     4621      39204  open64                    \n",
      "     0.0           216230          3     72076.7    69176      76615  fgets                     \n",
      "     0.0           181725          4     45431.3    36634      53984  pthread_create            \n",
      "     0.0           169084         25      6763.4     1566      35012  fopen                     \n",
      "     0.0            86333         11      7848.5     4734      13032  write                     \n",
      "     0.0            56315         11      5119.5     2581      11434  munmap                    \n",
      "     0.0            46391         30      1546.4     1013       9538  fcntl                     \n",
      "     0.0            35663          5      7132.6     3216       9293  open                      \n",
      "     0.0            34412         18      1911.8     1065       7409  fclose                    \n",
      "     0.0            27661          6      4610.2     1096      10820  fgetc                     \n",
      "     0.0            23708         13      1823.7     1074       3151  read                      \n",
      "     0.0            20163          2     10081.5     9571      10592  socket                    \n",
      "     0.0            18043          3      6014.3     3238       7906  fread                     \n",
      "     0.0            17573          1     17573.0    17573      17573  sem_wait                  \n",
      "     0.0            17163          1     17163.0    17163      17163  pthread_rwlock_timedwrlock\n",
      "     0.0             9953          4      2488.3     1796       3178  mprotect                  \n",
      "     0.0             9624          1      9624.0     9624       9624  connect                   \n",
      "     0.0             8745          1      8745.0     8745       8745  pipe2                     \n",
      "     0.0             2862          1      2862.0     2862       2862  bind                      \n",
      "     0.0             1733          1      1733.0     1733       1733  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report2.qdrep\"\n",
      "Report file moved to \"/dli/task/report2.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Blocks,Threads | CUDA Time |\n",
    "| --- | --- |  \n",
    "| 1,16 | 575574928 |  \n",
    "| 2,16 |  303114205 |\n",
    "| 8,16 | 171634643 | \n",
    "| 2,32 |  282598547 |  \n",
    "| 4,32 | 276147942 |  \n",
    "| 2,64 |  181480407 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-547f-2b3e-50b5-173f.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-547f-2b3e-50b5-173f.qdrep\"\n",
      "Exporting 4260 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-547f-2b3e-50b5-173f.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    56.3        247995189          3   82665063.0      18384  247925951  cudaMallocManaged    \n",
      "    39.0        171642830          1  171642830.0  171642830  171642830  cudaDeviceSynchronize\n",
      "     4.7         20911856          3    6970618.7    6286577    8060870  cudaFree             \n",
      "     0.0            42556          1      42556.0      42556      42556  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        171634643          1  171634643.0  171634643  171634643  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.5         68550212        2304  29752.7     1951   171327  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.5         21043109         768  27399.9     1311   159807  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    85.1       1453913191         75  19385509.2    36827  100141870  poll                      \n",
      "     7.7        130965149         64   2046330.5    20724   20853693  sem_timedwait             \n",
      "     5.7         96514882        684    141103.6     1065   20835415  ioctl                     \n",
      "     1.4         23713024         98    241969.6     1586    7991625  mmap                      \n",
      "     0.1          1954979         82     23841.2     7148      41455  open64                    \n",
      "     0.0           305026          3    101675.3    99285     105822  fgets                     \n",
      "     0.0           196461          4     49115.3    41307      60112  pthread_create            \n",
      "     0.0           164729         25      6589.2     2097      26909  fopen                     \n",
      "     0.0           104086         11      9462.4     5712      15214  write                     \n",
      "     0.0            56882         12      4740.2     1751       8824  munmap                    \n",
      "     0.0            39920          5      7984.0     4474      11207  open                      \n",
      "     0.0            35296         18      1960.9     1296       5039  fclose                    \n",
      "     0.0            31674         13      2436.5     1333       3727  read                      \n",
      "     0.0            28480         19      1498.9     1004       6494  fcntl                     \n",
      "     0.0            27357          6      4559.5     1435      12041  fgetc                     \n",
      "     0.0            19214          2      9607.0     7427      11787  socket                    \n",
      "     0.0            13350          1     13350.0    13350      13350  connect                   \n",
      "     0.0            13057          4      3264.3     2352       4486  mprotect                  \n",
      "     0.0            10701          3      3567.0     1850       4832  fread                     \n",
      "     0.0             9378          2      4689.0     1277       8101  pthread_rwlock_timedwrlock\n",
      "     0.0             7856          1      7856.0     7856       7856  pipe2                     \n",
      "     0.0             3372          1      3372.0     3372       3372  bind                      \n",
      "     0.0             2020          1      2020.0     2020       2020  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report7.qdrep\"\n",
      "Report file moved to \"/dli/task/report7.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-e869-fd08-5989-f68e.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-e869-fd08-5989-f68e.qdrep\"\n",
      "Exporting 4236 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-e869-fd08-5989-f68e.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    56.2        226648231          3   75549410.3      16989  226589174  cudaMallocManaged    \n",
      "    38.6        155515391          1  155515391.0  155515391  155515391  cudaDeviceSynchronize\n",
      "     5.2         21064296          3    7021432.0    6301892    8271288  cudaFree             \n",
      "     0.0            60061          1      60061.0      60061      60061  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        155507628          1  155507628.0  155507628  155507628  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.5         68866362        2304  29889.9     2175   171199  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.5         21213272         768  27621.4     1598   160030  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    84.3       1343842830         73  18408805.9    26069  100126819  poll                      \n",
      "     8.8        140667776         65   2164119.6    19056   20835897  sem_timedwait             \n",
      "     5.2         83147478        676    122999.2     1003   17007441  ioctl                     \n",
      "     1.5         23388870         98    238661.9     1265    8211205  mmap                      \n",
      "     0.1          1367392         82     16675.5     4380      24405  open64                    \n",
      "     0.0           213719          3     71239.7    69533      74587  fgets                     \n",
      "     0.0           138362          4     34590.5    30700      37613  pthread_create            \n",
      "     0.0           115435         25      4617.4     1533      22701  fopen                     \n",
      "     0.0            78183         11      7107.5     4369      10872  write                     \n",
      "     0.0            40704         11      3700.4     1593       5226  munmap                    \n",
      "     0.0            35061         18      1947.8     1078       9808  fclose                    \n",
      "     0.0            27749          5      5549.8     3086       8737  open                      \n",
      "     0.0            26278          7      3754.0     1052       7341  fgetc                     \n",
      "     0.0            24529         13      1886.8     1295       3457  read                      \n",
      "     0.0            11896          1     11896.0    11896      11896  pipe2                     \n",
      "     0.0            10770          2      5385.0     5261       5509  socket                    \n",
      "     0.0            10475          2      5237.5     1000       9475  pthread_rwlock_timedwrlock\n",
      "     0.0             9877          3      3292.3     1491       4481  fread                     \n",
      "     0.0             8907          4      2226.8     1065       5520  fcntl                     \n",
      "     0.0             8071          4      2017.8     1701       2354  mprotect                  \n",
      "     0.0             6682          1      6682.0     6682       6682  connect                   \n",
      "     0.0             2297          1      2297.0     2297       2297  bind                      \n",
      "     0.0             1645          1      1645.0     1645       1645  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report8.qdrep\"\n",
      "Report file moved to \"/dli/task/report8.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if memory calls happen before they're initialized, page faults are triggered which slow down execution  \n",
    "Hence async prefetching is preferred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-4879-46fe-95a5-6827.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-4879-46fe-95a5-6827.qdrep\"\n",
      "Exporting 1042 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-4879-46fe-95a5-6827.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    80.9        227137321          1  227137321.0  227137321  227137321  cudaMallocManaged    \n",
      "    16.9         47518376          1   47518376.0   47518376   47518376  cudaDeviceSynchronize\n",
      "     2.2          6153137          1    6153137.0    6153137    6153137  cudaFree             \n",
      "     0.0            32557          1      32557.0      32557      32557  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum            Name          \n",
      " -------  ---------------  ---------  ----------  --------  --------  -----------------------\n",
      "   100.0         47512482          1  47512482.0  47512482  47512482  deviceKernel(int*, int)\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    72.8        391649699         23  17028247.8    29840  100124124  poll                      \n",
      "    15.9         85520694        665    128602.5     1013   17184122  ioctl                     \n",
      "     9.3         49879812         18   2771100.7    15404   20506426  sem_timedwait             \n",
      "     1.6          8564250         92     93089.7     1451    6047923  mmap                      \n",
      "     0.3          1649873         82     20120.4     6098      26985  open64                    \n",
      "     0.0           205965          3     68655.0    67235      71173  fgets                     \n",
      "     0.0           135604          4     33901.0    32143      37177  pthread_create            \n",
      "     0.0           107987         25      4319.5     1444      22464  fopen                     \n",
      "     0.0            79541         11      7231.0     4453      11081  write                     \n",
      "     0.0            26240         18      1457.8     1040       3860  fclose                    \n",
      "     0.0            25242          5      5048.4     3096       7844  open                      \n",
      "     0.0            25109          7      3587.0     2236       4734  munmap                    \n",
      "     0.0            20114         13      1547.2     1032       2129  read                      \n",
      "     0.0            18396          2      9198.0     6566      11830  pthread_rwlock_timedwrlock\n",
      "     0.0            17737          6      2956.2     1004       9663  fcntl                     \n",
      "     0.0            17031          5      3406.2     1058       6918  fgetc                     \n",
      "     0.0            10239          1     10239.0    10239      10239  sem_wait                  \n",
      "     0.0             9061          2      4530.5     4318       4743  socket                    \n",
      "     0.0             7886          1      7886.0     7886       7886  pipe2                     \n",
      "     0.0             7212          4      1803.0     1708       1937  mprotect                  \n",
      "     0.0             6714          2      3357.0     3199       3515  fread                     \n",
      "     0.0             6546          1      6546.0     6546       6546  connect                   \n",
      "     0.0             2020          1      2020.0     2020       2020  bind                      \n",
      "     0.0             1606          1      1606.0     1606       1606  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report14.qdrep\"\n",
      "Report file moved to \"/dli/task/report14.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-ec75-ceba-b524-82d5.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-ec75-ceba-b524-82d5.qdrep\"\n",
      "Exporting 4302 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-ec75-ceba-b524-82d5.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    65.9        331957075          3  110652358.3      17739  331891204  cudaMallocManaged    \n",
      "    29.9        150612014          1  150612014.0  150612014  150612014  cudaDeviceSynchronize\n",
      "     4.2         21341981          3    7113993.7    6256429    8654944  cudaFree             \n",
      "     0.0           109813          1     109813.0     109813     109813  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        150661751          1  150661751.0  150661751  150661751  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         69141723        2304  30009.4     2206   182559  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         21143744         768  27530.9     1407   166688  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    81.4       1445604975         74  19535202.4    44861  100132175  poll                      \n",
      "     8.6        151923526        684    222110.4     1003   30757660  ioctl                     \n",
      "     8.4        148649767         64   2322652.6    13526   20509218  sem_timedwait             \n",
      "     1.4         25405814         98    259243.0     1410    8601022  mmap                      \n",
      "     0.1          2114326         82     25784.5     8462      48231  open64                    \n",
      "     0.0           424242          3    141414.0   136411     149114  fgets                     \n",
      "     0.0           249718          4     62429.5    51350      76558  pthread_create            \n",
      "     0.0           188881         25      7555.2     2698      25941  fopen                     \n",
      "     0.0           152556         11     13868.7     7800      29471  write                     \n",
      "     0.0            81632          7     11661.7     7241      17477  pthread_rwlock_timedwrlock\n",
      "     0.0            79276         60      1321.3     1006       3877  fcntl                     \n",
      "     0.0            64771         12      5397.6     1859       9760  munmap                    \n",
      "     0.0            53525          5     10705.0     6216      16085  open                      \n",
      "     0.0            42166          6      7027.7     1830      20436  fgetc                     \n",
      "     0.0            41688         18      2316.0     1461       4958  fclose                    \n",
      "     0.0            26943          2     13471.5    11138      15805  socket                    \n",
      "     0.0            22016         10      2201.6     1020       4580  read                      \n",
      "     0.0            16382          1     16382.0    16382      16382  connect                   \n",
      "     0.0            13936          4      3484.0     3131       4046  mprotect                  \n",
      "     0.0            13626          1     13626.0    13626      13626  pipe2                     \n",
      "     0.0             9909          3      3303.0     1413       4777  fread                     \n",
      "     0.0             4190          1      4190.0     4190       4190  bind                      \n",
      "     0.0             2923          1      2923.0     2923       2923  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report15.qdrep\"\n",
      "Report file moved to \"/dli/task/report15.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-17cf-a49c-1c00-fe55.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-17cf-a49c-1c00-fe55.qdrep\"\n",
      "Exporting 1889 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-17cf-a49c-1c00-fe55.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    59.7        226387797          3  75462599.0     19006  226296743  cudaMallocManaged    \n",
      "    35.2        133422006          2  66711003.0  31506445  101915561  cudaDeviceSynchronize\n",
      "     5.0         19130360          3   6376786.7   5440157    8073427  cudaFree             \n",
      "     0.0            92662          4     23165.5      7079      40087  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    76.4        101916940          3  33972313.3  33336872  34946686  initWith(float, float*, int)               \n",
      "    23.6         31500468          1  31500468.0  31500468  31500468  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21232097         768  27646.0     1503   173055  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    79.5        783169507         44  17799307.0    32723  100126842  poll                      \n",
      "     9.6         94303154         38   2481661.9    17178   20553347  sem_timedwait             \n",
      "     8.4         83101036        675    123112.6     1058   17181403  ioctl                     \n",
      "     2.2         21543054         98    219827.1     1364    8020431  mmap                      \n",
      "     0.2          1802405         82     21980.5     6043      36026  open64                    \n",
      "     0.0           238126          3     79375.3    69904      92520  fgets                     \n",
      "     0.0           167081          4     41770.3    32234      51489  pthread_create            \n",
      "     0.0           145971         25      5838.8     1657      24476  fopen                     \n",
      "     0.0            89508         11      8137.1     4015      14056  write                     \n",
      "     0.0            51272          6      8545.3     1058      26627  fgetc                     \n",
      "     0.0            50777         11      4616.1     1903       9683  munmap                    \n",
      "     0.0            44040          4     11010.0     6836      17618  pthread_rwlock_timedwrlock\n",
      "     0.0            34391          5      6878.2     3092      11637  open                      \n",
      "     0.0            31305         18      1739.2     1088       5041  fclose                    \n",
      "     0.0            23102         13      1777.1     1026       3005  read                      \n",
      "     0.0            18694          2      9347.0     7780      10914  socket                    \n",
      "     0.0            12596          8      1574.5     1021       4579  fcntl                     \n",
      "     0.0            10836          3      3612.0     2039       4961  fread                     \n",
      "     0.0            10428          1     10428.0    10428      10428  connect                   \n",
      "     0.0             9912          1      9912.0     9912       9912  sem_wait                  \n",
      "     0.0             8651          1      8651.0     8651       8651  pipe2                     \n",
      "     0.0             8592          4      2148.0     1874       2771  mprotect                  \n",
      "     0.0             2706          1      2706.0     2706       2706  bind                      \n",
      "     0.0             1753          1      1753.0     1753       1753  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report17.qdrep\"\n",
      "Report file moved to \"/dli/task/report17.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-cdee-25cf-042c-5e3a.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-cdee-25cf-042c-5e3a.qdrep\"\n",
      "Exporting 1895 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-cdee-25cf-042c-5e3a.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    68.2        236431548          3  78810516.0     36452  236287300  cudaMallocManaged    \n",
      "    25.6         88568210          2  44284105.0  28756925   59811285  cudaDeviceSynchronize\n",
      "     5.9         20295497          3   6765165.7   5932861    8250560  cudaFree             \n",
      "     0.3          1203963          3    401321.0    252045     679284  cudaMemPrefetchAsync \n",
      "     0.0           107021          4     26755.3     10278      52039  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    67.5         59825310          3  19941770.0  19843467  20092201  initWith(float, float*, int)               \n",
      "    32.5         28750664          1  28750664.0  28750664  28750664  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21372795         768  27829.2     1631   177950  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    80.0        731661685         39  18760556.0    46405  100129129  poll                      \n",
      "    10.2         93292659        679    137397.1     1067   17209659  ioctl                     \n",
      "     7.0         64468381         32   2014636.9    14407   20583497  sem_timedwait             \n",
      "     2.5         22728087         98    231919.3     1520    8185998  mmap                      \n",
      "     0.2          1951161         82     23794.6     6467      48996  open64                    \n",
      "     0.0           237394          3     79131.3    70333      93976  fgets                     \n",
      "     0.0           157232          4     39308.0    32495      44142  pthread_create            \n",
      "     0.0           128677         25      5147.1     1514      22707  fopen                     \n",
      "     0.0           100980         11      9180.0     4494      13339  write                     \n",
      "     0.0            52747         11      4795.2     2465      10707  munmap                    \n",
      "     0.0            35692          5      7138.4     4044       9524  open                      \n",
      "     0.0            32506          6      5417.7     1292      11297  fgetc                     \n",
      "     0.0            32051         22      1456.9     1039       6233  fcntl                     \n",
      "     0.0            29114         18      1617.4     1080       4895  fclose                    \n",
      "     0.0            22220         12      1851.7     1072       3073  read                      \n",
      "     0.0            21152          2     10576.0     8807      12345  pthread_rwlock_timedwrlock\n",
      "     0.0            14515          2      7257.5     5443       9072  socket                    \n",
      "     0.0            11407          3      3802.3     2263       4993  fread                     \n",
      "     0.0             9182          4      2295.5     1899       3047  mprotect                  \n",
      "     0.0             8428          1      8428.0     8428       8428  connect                   \n",
      "     0.0             7562          1      7562.0     7562       7562  pipe2                     \n",
      "     0.0             2441          1      2441.0     2441       2441  bind                      \n",
      "     0.0             1680          1      1680.0     1680       1680  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report19.qdrep\"\n",
      "Report file moved to \"/dli/task/report19.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-73f6-bdd4-c116-30d1.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-73f6-bdd4-c116-30d1.qdrep\"\n",
      "Exporting 1752 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-73f6-bdd4-c116-30d1.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    55.7        221799035          3  73933011.7     17919  221742809  cudaMallocManaged    \n",
      "    31.6        126125003          2  63062501.5  29219850   96905153  cudaDeviceSynchronize\n",
      "     7.8         31076839          6   5179473.2    138947   10244116  cudaMemPrefetchAsync \n",
      "     4.9         19459465          3   6486488.3   5741033    7921085  cudaFree             \n",
      "     0.0            60203          4     15050.8      4677      25377  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    76.8         96907358          3  32302452.7  32024140  32553160  initWith(float, float*, int)               \n",
      "    23.2         29217857          1  29217857.0  29217857  29217857  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         31373165         624  50277.5     1375   332701  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 196608.000         624  315.077    4.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    77.5        782874518         44  17792602.7    30938  100123580  poll          \n",
      "    11.1        112333183        683    164470.3     1001   17077818  ioctl         \n",
      "     9.1         91465437         38   2406985.2    12696   20503739  sem_timedwait \n",
      "     2.1         21699046         98    221418.8     1437    7862458  mmap          \n",
      "     0.1          1343951         82     16389.6     4269      23733  open64        \n",
      "     0.0           208389          3     69463.0    67494      73156  fgets         \n",
      "     0.0           135001          4     33750.3    31785      35578  pthread_create\n",
      "     0.0           106881         25      4275.2     1589      21156  fopen         \n",
      "     0.0            77865         11      7078.6     3997      10999  write         \n",
      "     0.0            38780         11      3525.5     1600       4995  munmap        \n",
      "     0.0            29788          7      4255.4     1184       8705  fgetc         \n",
      "     0.0            28158          5      5631.6     3012       8796  open          \n",
      "     0.0            26658         18      1481.0     1037       4229  fclose        \n",
      "     0.0            19896         13      1530.5     1043       2266  read          \n",
      "     0.0            13792          1     13792.0    13792      13792  sem_wait      \n",
      "     0.0             9291          3      3097.0     1658       4133  fread         \n",
      "     0.0             8978          2      4489.0     4056       4922  socket        \n",
      "     0.0             7684          1      7684.0     7684       7684  pipe2         \n",
      "     0.0             7470          4      1867.5     1674       2205  mprotect      \n",
      "     0.0             7183          4      1795.8     1000       4015  fcntl         \n",
      "     0.0             5380          1      5380.0     5380       5380  connect       \n",
      "     0.0             1981          1      1981.0     1981       1981  bind          \n",
      "     0.0             1473          1      1473.0     1473       1473  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report20.qdrep\"\n",
      "Report file moved to \"/dli/task/report20.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](../edit/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nsys profile`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200us*. Check out [the solution](../edit/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \r\n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-2c58-ccd3-b0d0-ae50.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-2c58-ccd3-b0d0-ae50.qdrep\"\n",
      "Exporting 1062 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-2c58-ccd3-b0d0-ae50.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum         Name       \n",
      " -------  ---------------  ---------  ----------  -------  ---------  -----------------\n",
      "    95.3        242216101          3  80738700.3    26070  242145985  cudaMallocManaged\n",
      "     4.7         11831474          3   3943824.7   723373   10331269  cudaFree         \n",
      "     0.0            39867          2     19933.5     7563      32304  cudaLaunchKernel \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum           Name          \n",
      " -------  ---------------  ---------  ---------  -------  -------  -----------------------\n",
      "    97.8          9464980          1  9464980.0  9464980  9464980  init(int*, int*, int*) \n",
      "     2.2           212158          1   212158.0   212158   212158  saxpy(int*, int*, int*)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0            25919           3   8639.7     5568    12288  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "  Total   Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ----------  -------  -------  -------  ---------------------------------\n",
      " 128.000           3   42.667   24.000   64.000  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    64.2        250488938         18  13916052.1    47430  100126612  poll                      \n",
      "    24.9         97321019        675    144179.3     1001   17106750  ioctl                     \n",
      "     9.1         35710568         15   2380704.5    11172   20834428  sem_timedwait             \n",
      "     1.2          4715119         97     48609.5     1412     750696  mmap                      \n",
      "     0.4          1410406         82     17200.1     4398      28921  open64                    \n",
      "     0.1           210715          3     70238.3    67752      73080  fgets                     \n",
      "     0.0           147033          4     36758.3    32257      41449  pthread_create            \n",
      "     0.0           132392         25      5295.7     1452      24119  fopen                     \n",
      "     0.0            82365         11      7487.7     4155      12730  write                     \n",
      "     0.0            33881          9      3764.6     2055       8500  munmap                    \n",
      "     0.0            30243          5      6048.6     3257       8763  open                      \n",
      "     0.0            28836         18      1602.0     1054       4841  fclose                    \n",
      "     0.0            26398         12      2199.8     1027       7604  read                      \n",
      "     0.0            21586          6      3597.7     1016       9171  fgetc                     \n",
      "     0.0            21018          3      7006.0     2507      12151  fread                     \n",
      "     0.0            16969          1     16969.0    16969      16969  pthread_rwlock_timedwrlock\n",
      "     0.0            15991         10      1599.1     1014       5320  fcntl                     \n",
      "     0.0            13310          2      6655.0     5718       7592  socket                    \n",
      "     0.0             8111          4      2027.8     1800       2500  mprotect                  \n",
      "     0.0             7971          1      7971.0     7971       7971  pipe2                     \n",
      "     0.0             7540          1      7540.0     7540       7540  connect                   \n",
      "     0.0             2538          1      2538.0     2538       2538  bind                      \n",
      "     0.0             1864          1      1864.0     1864       1864  sem_wait                  \n",
      "     0.0             1685          1      1685.0     1685       1685  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report21.qdrep\"\n",
      "Report file moved to \"/dli/task/report21.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
