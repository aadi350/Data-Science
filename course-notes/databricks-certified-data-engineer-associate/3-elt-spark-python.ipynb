{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELT With Spark\n",
    "Querying files of file paths:  \n",
    "```SQL\n",
    "select * from file_format.\\`/path/to/file\\`\n",
    "```\n",
    "\n",
    "`file_format` can be `csv`, `json`, etc  \n",
    "\n",
    "\n",
    "```SQL \n",
    "select * from text.`/path/to/file` -- when file might be corrupted\n",
    "```\n",
    "\n",
    "CTAS statement:  \n",
    "\n",
    "```sql\n",
    "CREATE TABLE table_name\n",
    "AS SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "\n",
    "CTAS doesn't support manual schema declration, usefulf only when source has its defined schema. Does not support file options (headers, separators, etc)\n",
    "\n",
    "\n",
    "This supports options, but there's no data movement. This is a NON-DELTA table, where the table is external, just referenced. Hence no time-travel, no performance benefits of delta.\n",
    "```sql\n",
    "CREATE TABLE table_name (col list)\n",
    "USING data_source\n",
    "OPTIONS (key1 = val1, key2 = val2, ...)\n",
    "LOCATION = path\n",
    "\n",
    "-- example\n",
    "CREATE TABLE table_name (col list)\n",
    "USING JDBC\n",
    "OPTIONS (\n",
    "  url=\"\",\n",
    "  dbtable=\"\",\n",
    "  username=\"\",\n",
    "  password=\"\"\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "Solution to the performance, first create a `TEMP VIEW` of external, then load as delta:\n",
    "```sql\n",
    "CREATE TEMP VIEW temp_view_name (col list)\n",
    "USING data_source\n",
    "OPTIONS (...)\n",
    "LOCATION = paath\n",
    "\n",
    "CREATE TABLE table_name\n",
    "AS SELECT * FROM temp_view_name -- creates the delta!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/28 08:36:00 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.237 instead (on interface wlp5s0)\n",
      "23/06/28 08:36:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/aadi/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aadi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aadi/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ca2bc352-e65a-4f58-ab7d-09eedd1cb450;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 100ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ca2bc352-e65a-4f58-ab7d-09eedd1cb450\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/28 08:36:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder.appName('delta-tutorial').config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work well unless schema is self-defined\n",
    "spark.sql('''\n",
    "select *, input_file_name() source_file  from csv.`/storage/data/airline_2m.csv` \n",
    "''').limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', 'true').csv('/storage/data/airline_2m.csv').select(*[\n",
    "    'Year', 'Month', 'FlightDate','Reporting_Airline', 'Origin', 'Dest'\n",
    "]).write.mode('overwrite').csv('/storage/data/airlines_2m_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/28 08:41:14 WARN HadoopFSUtils: The directory file:/storage/data/airline_2m_small.csv was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE airline')\n",
    "spark.sql('''\n",
    "CREATE TABLE airline \n",
    "  (FlightDate timestamp, Reporting_Airline string, Flight_Number_Reporting_Airline int, Origin string, Dest string, DepTime int, DepDelay double, ArrTime int, ArrDelay double)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  header=\"true\",\n",
    "  delimiter=\";\"\n",
    ")\n",
    "LOCATION \"/storage/data/airline_2m_small.csv\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----------------------------------------+-------+\n",
      "|col_name                       |data_type                                |comment|\n",
      "+-------------------------------+-----------------------------------------+-------+\n",
      "|FlightDate                     |timestamp                                |null   |\n",
      "|Reporting_Airline              |string                                   |null   |\n",
      "|Flight_Number_Reporting_Airline|int                                      |null   |\n",
      "|Origin                         |string                                   |null   |\n",
      "|Dest                           |string                                   |null   |\n",
      "|DepTime                        |int                                      |null   |\n",
      "|DepDelay                       |double                                   |null   |\n",
      "|ArrTime                        |int                                      |null   |\n",
      "|ArrDelay                       |double                                   |null   |\n",
      "|                               |                                         |       |\n",
      "|# Detailed Table Information   |                                         |       |\n",
      "|Database                       |default                                  |       |\n",
      "|Table                          |airline                                  |       |\n",
      "|Created Time                   |Wed Jun 28 08:41:14 AST 2023             |       |\n",
      "|Last Access                    |UNKNOWN                                  |       |\n",
      "|Created By                     |Spark 3.3.2                              |       |\n",
      "|Type                           |EXTERNAL                                 |       |\n",
      "|Provider                       |CSV                                      |       |\n",
      "|Location                       |file:///storage/data/airline_2m_small.csv|       |\n",
      "|Storage Properties             |[header=true, delimiter=;]               |       |\n",
      "+-------------------------------+-----------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no data moved during table creation, Location is still CSV (external)\n",
    "# all metadata stored in metastore\n",
    "spark.sql('describe extended airline').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/28 08:47:02 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`airline`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n+- Project [_c0#2155, _c1#2156, _c2#2157, _c3#2158, _c4#2159, _c5#2160, _c6#2161, _c7#2162, _c8#2163, _c9#2164, _c10#2165, _c11#2166, _c12#2167, _c13#2168, _c14#2169, _c15#2170, _c16#2171, _c17#2172, _c18#2173, _c19#2174, _c20#2175, _c21#2176, _c22#2177, _c23#2178, ... 85 more fields]\n   +- Relation [_c0#2155,_c1#2156,_c2#2157,_c3#2158,_c4#2159,_c5#2160,_c6#2161,_c7#2162,_c8#2163,_c9#2164,_c10#2165,_c11#2166,_c12#2167,_c13#2168,_c14#2169,_c15#2170,_c16#2171,_c17#2172,_c18#2173,_c19#2174,_c20#2175,_c21#2176,_c22#2177,_c23#2178,... 85 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# spark.sql('DROP TABLE airline')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m'''\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[39mCREATE TABLE airline \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[39mAS SELECT * FROM \u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[39mcsv.`/storage/data/airline_2m.csv`\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[39m'''\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[39m=\u001b[39m formatter\u001b[39m.\u001b[39mformat(sqlQuery, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`airline`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n+- Project [_c0#2155, _c1#2156, _c2#2157, _c3#2158, _c4#2159, _c5#2160, _c6#2161, _c7#2162, _c8#2163, _c9#2164, _c10#2165, _c11#2166, _c12#2167, _c13#2168, _c14#2169, _c15#2170, _c16#2171, _c17#2172, _c18#2173, _c19#2174, _c20#2175, _c21#2176, _c22#2177, _c23#2178, ... 85 more fields]\n   +- Relation [_c0#2155,_c1#2156,_c2#2157,_c3#2158,_c4#2159,_c5#2160,_c6#2161,_c7#2162,_c8#2163,_c9#2164,_c10#2165,_c11#2166,_c12#2167,_c13#2168,_c14#2169,_c15#2170,_c16#2171,_c17#2172,_c18#2173,_c19#2174,_c20#2175,_c21#2176,_c22#2177,_c23#2178,... 85 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DROP TABLE airline')\n",
    "spark.sql('''\n",
    "CREATE TABLE airline \n",
    "AS SELECT * FROM \n",
    "csv.`/storage/data/airline_2m.csv`\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{                                                                                                                                                                                                                  |\n",
      "|  \"meta\" : {                                                                                                                                                                                                       |\n",
      "|    \"view\" : {                                                                                                                                                                                                     |\n",
      "|      \"id\" : \"f6w7-q2d2\",                                                                                                                                                                                          |\n",
      "|      \"name\" : \"Electric Vehicle Population Data\",                                                                                                                                                                 |\n",
      "|      \"assetType\" : \"dataset\",                                                                                                                                                                                     |\n",
      "|      \"attribution\" : \"Washington State Department of Licensing\",                                                                                                                                                  |\n",
      "|      \"averageRating\" : 0,                                                                                                                                                                                         |\n",
      "|      \"category\" : \"Transportation\",                                                                                                                                                                               |\n",
      "|      \"createdAt\" : 1555435581,                                                                                                                                                                                    |\n",
      "|      \"description\" : \"This dataset shows the Battery Electric Vehicles (BEVs) and Plug-in Hybrid Electric Vehicles (PHEVs) that are currently registered through Washington State Department of Licensing (DOL).\",|\n",
      "|      \"displayType\" : \"table\",                                                                                                                                                                                     |\n",
      "|      \"downloadCount\" : 101096,                                                                                                                                                                                    |\n",
      "|      \"hideFromCatalog\" : false,                                                                                                                                                                                   |\n",
      "|      \"hideFromDataJson\" : false,                                                                                                                                                                                  |\n",
      "|      \"licenseId\" : \"ODBL\",                                                                                                                                                                                        |\n",
      "|      \"newBackend\" : true,                                                                                                                                                                                         |\n",
      "|      \"numberOfComments\" : 0,                                                                                                                                                                                      |\n",
      "|      \"oid\" : 38591322,                                                                                                                                                                                            |\n",
      "|      \"provenance\" : \"official\",                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select * from text.`/storage/data/electric-vehicle.json`\n",
    "''').show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac5f9a12ce42e3ce4a716d69b8738cd831a51d5f24bd9d0d377d51220bf4645"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
